{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note: this notebook requires pandas 0.21.0 or newer\n",
    "from rfpimp import *\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shared_functions as sf\n",
    "import math\n",
    "from datetime import datetime as dt\n",
    "import re as re\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic EDA\n",
    "# The training data needs to be saved in the same location.\n",
    "original_data = pd.read_csv('train.csv')\n",
    "data = original_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background to the problem (Hilary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering (YW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove features with very little variation and those that are missing for most observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(original_data.isnull(), cbar=False)\n",
    "data = data.drop(columns=['Alley', 'PoolQC', 'Fence', 'MiscFeature'])\n",
    "\n",
    "# Drop variables with very imblanced categories or mostly missing\n",
    "data = data.drop(columns = ['Street', 'Utilities', 'LowQualFinSF', 'MiscVal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove features that are redundant given other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "### BASEMENTSF ###\n",
    "# Drop the aggregate BsmtSF variable\n",
    "test = pd.DataFrame()\n",
    "test['DiffBsmtSF'] = original_data[\"TotalBsmtSF\"] - original_data['BsmtFinSF1'] - original_data['BsmtUnfSF'] - original_data['BsmtFinSF2']\n",
    "test['DiffBsmtSF'].value_counts()\n",
    "\n",
    "# Remove variable\n",
    "data = data.drop(columns=['TotalBsmtSF'])\n",
    "\n",
    "##################\n",
    "### GRLIVAREA ###\n",
    "# Is GrLivArea simply the sum of 1st and 2nd?\n",
    "# Most of hte time it is - there are 5 observations that are not, but doesn't seem like a goood reason to keep\n",
    "test = pd.DataFrame()\n",
    "test['GrLivArea'] = original_data[\"GrLivArea\"]\n",
    "test['1stAnd2nd'] = original_data[\"1stFlrSF\"] + original_data[\"2ndFlrSF\"]\n",
    "test['Diff'] = test['GrLivArea'] - test['1stAnd2nd']\n",
    "test['Diff'].value_counts()\n",
    "\n",
    "###################\n",
    "### GARAGECARS ###\n",
    "data = data.drop(columns = ['GrLivArea'])\n",
    "original_data['GarageCars'].value_counts()\n",
    "plt.scatter(original_data['GarageCars'],original_data['GarageArea'])\n",
    "#data = data.drop(columns=['GarageCars']) # KEEP FOR NOW, but garage area may be better than garage cars\n",
    "\n",
    "###################\n",
    "### DATETIME    ###\n",
    "# Change date to ordinal representation:\n",
    "data['YrMoSold'] = data['YrSold'].astype(int).astype(str) + \"/\" + data['MoSold'].astype(int).astype(str) + \"/1\"\n",
    "data['YrMoSold'] = data['YrMoSold'].apply(lambda x: dt.strptime(x, \"%Y/%m/%d\").date().toordinal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "# Impute all numeric missings (all dummy missings are coded 0)\n",
    "imp = SimpleImputer(missing_values = np.nan, strategy = 'median')\n",
    "data_numeric = imp.fit_transform(data.select_dtypes(exclude='object'))\n",
    "data_numeric = pd.DataFrame(data_numeric, columns = data.select_dtypes(exclude='object').columns)\n",
    "data = pd.concat([data.select_dtypes(include='object'), data_numeric], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine features that are redundant given other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine categories - very thin categories\n",
    "# Condition1\n",
    "data['Condition1'] = np.where(data['Condition1']=='Norm', 'Norm', 'Not_Norm')\n",
    "data['Condition2'] = np.where(data['Condition2']=='Norm', 'Norm', 'Not_Norm')\n",
    "\n",
    "# Exterior1st\n",
    "data['Exterior1st'] = np.where(data['Exterior1st'].isin(['BrkComm', 'Stone', 'AsphShn', 'ImStucc', \"CBlock\", \"WdShing\", \"Stucco\", \"AsbShng\"]), 'Other', data['Exterior1st'])\n",
    "data['Exterior2nd'] = np.where(data['Exterior2nd'].isin(['Brk Cmn', 'Stone', 'AsphShn', 'Other', \"CBlock\", \"WdShing\", \"Stucco\", \"AsbShng\"]), 'Other', data['Exterior1st'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Binarize all categorical (one-hot)\n",
    "data = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simplify the Condition Feature\n",
    "data['Condition_Norm'] = data['Condition1_Norm'] + data['Condition2_Norm']\n",
    "data['Condition_Not_Norm'] = data['Condition1_Not_Norm'] + data['Condition2_Not_Norm']\n",
    "data['Condition_Norm'] = np.where(data['Condition_Norm']==2, 1, data['Condition_Norm'])\n",
    "data['Condition_Not_Norm'] = np.where(data['Condition_Not_Norm']==2, 1, data['Condition_Not_Norm'])\n",
    "\n",
    "# Simplify the Exterior Feature\n",
    "data['Exterior_VinylSd'] = data['Exterior1st_VinylSd'] + data['Exterior2nd_VinylSd']\n",
    "data['Exterior_HdBoard'] = data['Exterior1st_HdBoard'] + data['Exterior2nd_HdBoard']\n",
    "data['Exterior_MetalSd'] = data['Exterior1st_MetalSd'] + data['Exterior2nd_MetalSd']\n",
    "data['Exterior_Wd Sdng'] = data['Exterior1st_Wd Sdng'] + data['Exterior2nd_Wd Sdng']\n",
    "data['Exterior_Plywood'] = data['Exterior1st_Plywood'] + data['Exterior2nd_Plywood']\n",
    "data['Exterior_Other']   = data['Exterior1st_Other']   + data['Exterior2nd_Other']\n",
    "data['Exterior_CemntBd'] = data['Exterior1st_CemntBd'] + data['Exterior2nd_CemntBd']\n",
    "data['Exterior_BrkFace'] = data['Exterior1st_BrkFace'] + data['Exterior2nd_BrkFace']\n",
    "\n",
    "data['Exterior_VinylSd'] = np.where(data['Exterior_VinylSd'] == 2, 1, data['Exterior_VinylSd'])\n",
    "data['Exterior_HdBoard'] = np.where(data['Exterior_HdBoard'] == 2, 1, data['Exterior_HdBoard'])\n",
    "data['Exterior_MetalSd'] = np.where(data['Exterior_MetalSd'] == 2, 1, data['Exterior_MetalSd'])\n",
    "data['Exterior_Wd Sdng'] = np.where(data['Exterior_Wd Sdng'] == 2, 1, data['Exterior_Wd Sdng'])\n",
    "data['Exterior_Plywood'] = np.where(data['Exterior_Plywood'] == 2, 1, data['Exterior_Plywood'])\n",
    "data['Exterior_Other'] = np.where(data['Exterior_Other'] == 2, 1, data['Exterior_Other'])\n",
    "data['Exterior_CemntBd'] = np.where(data['Exterior_CemntBd'] == 2, 1, data['Exterior_CemntBd'])\n",
    "data['Exterior_BrkFace'] = np.where(data['Exterior_BrkFace'] == 2, 1, data['Exterior_BrkFace'])\n",
    "\n",
    "cols_to_drop = list(filter(lambda x: re.search(r'Condition1|Condition2|Exterior1|Exterior2', x), data.columns))\n",
    "data = data.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic feature importance to sense check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(20,10))\n",
    "corr = original_data.select_dtypes(exclude='object').corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .7})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use an algorithmic Feature Selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_variables = ['SalePrice']\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(data.drop(columns=['SalePrice']), data['SalePrice'])\n",
    "rf.feature_importances_\n",
    "\n",
    "feature_importances = pd.DataFrame(rf.feature_importances_.round(4),\n",
    "                                   index = data.drop(columns=['SalePrice']).columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "feature_importances[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Simple Version with fewer variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create a really simple version with 10 variables\n",
    "most_impt_cols = list(feature_importances[:10].index)\n",
    "most_impt_cols.append('SalePrice')\n",
    "\n",
    "def get_simple_data(input_data, colnames):\n",
    "    output_data = input_data[colnames]\n",
    "    return(output_data)\n",
    "    \n",
    "data = get_simple_data(data, most_impt_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "train_labels = data['SalePrice']\n",
    "\n",
    "# Data\n",
    "split = 500\n",
    "train_data = data.drop(columns=['SalePrice']).copy()\n",
    "\n",
    "# Split train data and dev Data\n",
    "dev_data = train_data[:split]\n",
    "train_data = train_data[split:]\n",
    "dev_labels = train_labels[:split]\n",
    "train_labels = train_labels[split:]\n",
    "\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridgemodel = Ridge(alpha=50)\n",
    "ridgemodel.fit(train_data, train_labels)\n",
    "dev_pred = ridgemodel.predict(dev_data)\n",
    "\n",
    "# Mean Abs % Error\n",
    "print(np.mean(np.abs(dev_pred-dev_labels)/dev_labels))\n",
    "\n",
    "# RMSLE\n",
    "dev_pred = np.where(dev_pred < 0, np.median(dev_pred), dev_pred) # sub median if 0\n",
    "sf.rmsle(dev_labels, dev_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RF\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(train_data, train_labels)\n",
    "dev_pred = rf.predict(dev_data)\n",
    "\n",
    "# Mean Abs % Error\n",
    "print(np.mean(np.abs(dev_pred-dev_labels)/dev_labels))\n",
    "\n",
    "# RMSLE\n",
    "sf.rmsle(dev_labels, dev_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Baseline & Early Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with a simple 15% dev set, but we have found that for this amount of data, the differences in the models and their scores on the dev sets can vary significantly based on which rows end up in the train and dev sets. Repeated random sub-sampling cross validation helps us get more consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# still to do : choose one version of pandas to use so that our code all agrees\n",
    "# and I don't have to read in a new dataset here \n",
    "data_df = pd.read_csv('clean_data.csv')\n",
    "data_df['LogSalePrice'] = np.log(data_df['SalePrice'])\n",
    "NUM_CROSS_VALS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the list of different cross val splits\n",
    "cross_val_list = []\n",
    "for i in range(NUM_CROSS_VALS):\n",
    "    split_idx = int(data_df.shape[0] * .85)\n",
    "    # line below is what shuffles\n",
    "    data_df = data_df.sample(frac=1)\n",
    "    train_df = data_df[:split_idx]\n",
    "    dev_df = data_df[split_idx:]\n",
    "    split_dict = {'train_df': train_df,\n",
    "                  'dev_df': dev_df}\n",
    "    cross_val_list.append(split_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our primary error metric, we focus on the root mean squared error of the logarithm of the prices, which is the error metric being used to create the leaderboard for this kaggle competition. See rmsle() in shared_functions.py for our implementation of the root mean squared error, an implementation we found from Mark Nagelberg on Kaggle: https://www.kaggle.com/marknagelberg/rmsle-function.\n",
    "\n",
    "When we consulted our resident real estate expert, Hilary's dad, about this problem, he told us that only one of these factors matters - \"location, location, location.\" In the spirit of that insight, we created a baseline \"model\" which looks at what neighborhood the house is in and takes the mean price of houses from that neighborhood in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: figure out why i'm getting nans now\n",
    "# when i wasn't in original notebook\n",
    "def baseline_pred(row,\n",
    "                  train_df):\n",
    "    for col in train_df:\n",
    "        if 'Neighborhood' in col:\n",
    "            if row[col] == 1:\n",
    "                neighborhood_var = col\n",
    "                break\n",
    "    return np.nanmean(train_df[train_df[neighborhood_var]==1]['LogSalePrice'])\n",
    "\n",
    "def get_baseline_cross_val(cross_val_list):\n",
    "    all_rmses = []\n",
    "    for di in cross_val_list:\n",
    "        dev_df = di['dev_df']\n",
    "        dev_df['baseline_pred'] = dev_df.apply(lambda row: baseline_pred(row,\n",
    "                                                                         di['train_df']), axis=1)\n",
    "        rmse = sf.rmsle(list(np.exp(dev_df['LogSalePrice'])), list(np.exp(dev_df['baseline_pred'])))\n",
    "        all_rmses.append(rmse)\n",
    "    return np.nanmean(all_rmses) \n",
    "\n",
    "# baseline RMSLE\n",
    "print(\"Baseline RMSLE: {:.3f}\".format(get_baseline_cross_val(cross_val_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this as a baseline, we began exploring how different types of models perform on the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "\n",
    "We begin with linear regression as the standard choice for a regression problem. In ordinary least squares regression, the regression line is fit by minimizing the sum of squared residuals between the predicted line and the true data points. We can interpret the resulting coefficients on each feature as representing the additional impact of a one-unit change in that feature on the final price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LinearRegression()]\n",
    "outcome_vars = ['LogSalePrice']\n",
    "feature_sets = [[col for col in data_df.columns if col not in ['YrMoSold', 'LogSalePrice', 'SalePrice']]]\n",
    "lrdf = sf.try_different_models(cross_val_list, models, outcome_vars, feature_sets)\n",
    "lrdf.sort_values('Root MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree-based regressors\n",
    "\n",
    "The family of tree-based regressors learns a series of simple decision rules to predict the final sale price. The decision tree regressor makes one single Decision Tree, whereas the Random Forest regressor trains an ensemble of decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = [DecisionTreeRegressor(), RandomForestRegressor()]\n",
    "df = sf.try_different_models(cross_val_list, models, outcome_vars, feature_sets)\n",
    "df[['Model', 'Num Features', 'Outcome Var', 'Root MSE']].sort_values('Root MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest regressor shows up in the table as the DecisionTreeRegressor with the parentheses around the whole function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Where this section is going)\n",
    "\n",
    "We intend to include more sections of different typess of models, and to build out the sections with some deeper explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "In this section, we'll go into more detail about how we actually iterated on models and chose whichever ones we end up deciding are our best. Our primary tools will be this error correlation table, which we'll use to look at patterns of errors the model is making, and diagnostics to determine whether or not the model is overfitting. We'll compare different models to each other and explain the model or ensemble that we chose as our \"best.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = lrdf.iloc[0]['Model']\n",
    "\n",
    "def create_error_correlation_table(model,\n",
    "                                   outcome_var,\n",
    "                                   feature_set,\n",
    "                                   dev_df):\n",
    "    \n",
    "    '''\n",
    "    finds correlation between absolute value of error\n",
    "    and each feature\n",
    "    '''\n",
    "    \n",
    "    dev_preds = model.predict(dev_df[feature_set])\n",
    "    dev_df = dev_df.reset_index()\n",
    "    \n",
    "    rmsles = []\n",
    "    for i in range(len(dev_preds)):\n",
    "        rmsles.append(sf.rmsle([dev_df[outcome_var][i]], [dev_preds[i]]))\n",
    "        \n",
    "    plt.clf()\n",
    "    plt.hist(rmsles, bins=20)\n",
    "    plt.xlabel(\"RMSLE\")\n",
    "    plt.ylabel(\"Number of Occurrences\")\n",
    "    plt.show()\n",
    "    \n",
    "    dev_df['linear_reg_errors'] = rmsles\n",
    "    \n",
    "    cols = []\n",
    "    corrs = []\n",
    "    for col in dev_df.columns:\n",
    "        try:\n",
    "            cor = np.corrcoef(abs(dev_df['linear_reg_errors']), dev_df[col])[0,1]\n",
    "            cols.append(col)\n",
    "            corrs.append(cor)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    corrs_df = pd.DataFrame(data={'col': cols, 'correlation': corrs})\n",
    "    corrs_df = corrs_df.dropna(subset=['correlation'])\n",
    "    return corrs_df\n",
    "    \n",
    "corrs_df = create_error_correlation_table(lr, 'LogSalePrice', feature_sets[-1], dev_df)\n",
    "corrs_df.reindex(corrs_df.correlation.abs().sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper Dive into modeling\n",
    "\n",
    "We intend to use this section to dive deeper into model interpretability, any insights we gained, and to talk through what types of models would be best for different use cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
