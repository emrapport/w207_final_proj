{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Baseline\n",
    "Yang Wei Neo, Emily Rapport, Hilary Yamtich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "import logging\n",
    "from rfpimp import *\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# note: this notebook requires pandas 0.21.0 or newer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shared_functions as sf\n",
    "import math\n",
    "from datetime import datetime as dt\n",
    "import re as re\n",
    "\n",
    "# For producing decision tree diagrams.\n",
    "from IPython.core.display import Image, display\n",
    "from sklearn.externals.six import StringIO\n",
    "\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "\n",
    "logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic EDA\n",
    "# The training data needs to be saved in the same location.\n",
    "original_data = pd.read_csv('train.csv')\n",
    "data = original_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background to the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset and Framing the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this problem is to use the Ames Housing Dataset to create a model to predict the sale prices of homes randomly selected from within the original dataset. \n",
    "\n",
    "The Ames Housing Dataset was compiled by Dean De Cock in 2011. It is based on all the housing sales that occured in Ames, Iowa during the period from 2006 to 2010. It was created as an alternative to the Boston Housing Dataset and it includes more feature variables (79) that are related to housing price. The original dataset included 2,390 observations. The goal of the creator was to include a wider set of explanatory variables than the original Boston Housing Dataset to allow for more advanced regression techniques to be applied.\n",
    "\n",
    "The problem itself is based on Kaggle competition to use advanced regression techniques to create a model to predict the sale price of a home. The test data is randomly selected from within the original Ames Housing Dataset. The training dataset used in the Kaggle competition includes 1,460 observations. The goal is to accurately predict the sale prices of the homes in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housing Price Trends over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the effect of the overall economy on housing prices within Ames, Iowa during this time period, we look at both the trends within our data and within the country as a whole. We start by looking at trends in our own data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = original_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = []\n",
    "\n",
    "for i in range(len(train_data[\"YrSold\"])):\n",
    "    dt = datetime.datetime(year = train_data[\"YrSold\"][i], month = train_data[\"MoSold\"][i], day = 1 )\n",
    "    dates.append(dt)\n",
    "\n",
    "train_data[\"DateSold\"] = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_medians = train_data.groupby('DateSold')['SalePrice'].median()\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(month_medians.index, month_medians)\n",
    "plt.title('Median Home Sale Prices by Month in Ames, Iowa')\n",
    "plt.xlabel('Time from 2006 to 2010')\n",
    "plt.ylabel('Median Prices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Looking at the medians by month, we do see sharp decline in 2008, which corresponds to the beginning of the housing crisis, and a peak right before the crisis. It is worth noting that the housing crisis may have effected housing prices in this data set. Comparing the data from Ames, Iowa over this time to the home prices in the nation over the same time period shows that the prices in Ames, Iowa appear to have been much more stable during the housing crisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below, we look at data from the Federal Reserve showing the S&P/Case-Schiller National Home Price Index. This helps us understand larger context of housing prices over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: fix the dates to be legible\n",
    "housing_data = pd.read_csv(\"housing_prices.csv\")\n",
    "plt.plot_date(x = housing_data['DATE'], \n",
    "              y = housing_data['CSUSHPINSA'], \n",
    "              xdate=True, \n",
    "              ydate=False, \n",
    "              data= housing_data)\n",
    "plt.title(\"Housing Price Index 1986 to 2018\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows a significant change in housing prices right around the time period we are looking at, from 2006 to 2010. Let's look at this portion of the graph in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data['DATE'] =  pd.to_datetime(housing_data['DATE'])\n",
    "housing_data[\"YEAR\"] = housing_data[\"DATE\"].dt.year\n",
    "\n",
    "housing_data_2006_2010 = housing_data[ (housing_data[\"YEAR\"] >= 2006) & (housing_data[\"YEAR\"] <= 2010)]\n",
    "plt.plot_date(x = housing_data_2006_2010['DATE'], \n",
    "              y = housing_data_2006_2010['CSUSHPINSA'], \n",
    "              xdate=True, \n",
    "              ydate=False, \n",
    "              data= housing_data_2006_2010)\n",
    "\n",
    "plt.title(\"Housing Price Index 2006 to 2010\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this analysis suggests that while there were significant trends in the overall US housing market during the 3 year window in with the data was collected, these trends might not have a significant impact on the data in our dataset. However, it is still worth considering how the economic climate overall would affect housing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Sale Price Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we look at the distribution of the sale price variable using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.hist(\"SalePrice\", bins = 50)\n",
    "plt.xlabel(\"Sale Prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"SalePrice\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the target variable above shows a right-hand skew with a few very large outliers. This initial display suggests that it might be possible to exclude some of the more expensive homes from the training data set, or to possibly create a model that makes different predictions based on whether the home falls into the average range or not.\n",
    "\n",
    "We can also look at the log of the sales prices to see how this changes the distribution of the target variable. The goal is to create a more normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"LogSalePrice\"] = np.log(train_data[\"SalePrice\"])\n",
    "train_data.hist(\"LogSalePrice\", bins = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a more normally distributed set of data. This makes our highly skewed distribution less skewed and allows our sale price data to meet the assumptions of inferential statistics and linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability vs Accuracy\n",
    "\n",
    "The goal of the Kaggle competiion is to create the most accurate model for predicting the sales price of the testing data. The model that produces the most accurate predictions might not necessarily be the most user-friendly or interpretable model for people actually seeking to use the model in the real world. For instance, real estate brokers or home buyers might be interested in a model that provided less accuracy in terms of exact predictions but more interpretability in terms of using it to make real-time decisions about how much to offer for a given home.\n",
    "\n",
    "An ideal solution to this problem would not only involve predicting future home prices, which would thus involve weighting recent sales more heavily than older sales, but would also involve various tiers of interpretability for different use cases. For instance, for lay-person home buyers or sellers trying to estimate the value of home, a very interpretable model with fewer included features would be more useful. However, for real estate professionals such as agents or developers, a more accurate model might be worth the additional computing power needed because these decision makers might have access to more resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have the original 79 features and the target of SalePrice, along with the ID column, the date and the log of the sale price. Each row represents a home that was sold in Ames, Iowa during the time period of 2006 through 2010. We also see below that there are 1,460 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the relationship of a few key variables to the target variable. First we do a basic check of the correlations between the numeric variables and the target variable to identify some key variables to explore. According to Random Forest, the most important features are the OverallQuality and the FirstFloorLiving Area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"SalePrice\", \"OverallQual\", \"1stFlrSF\"]\n",
    "pd.scatter_matrix(train_data[attributes], figsize=(6,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scatter matrix shows a strong correlation between the key variables of interest and the target variable, which is what we would expect. For instance, in the scatterplot of Overall Quality vs Sale Price, we see that increases in Overall Quality lead to clear increases in the Sale Price, with some exceptions for houses with the high quality--there appears to be a bigger range of sale prices for home with higher quality ratings. There is a similar relationship between the 1st Floor Square Feet and the Sale Price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "We have two broad approaches to feature engineering - 'top down' and 'bottom up'. \n",
    "\n",
    "In the 'top down' case, we start with all the variables and work to eliminate / simplify variables as much as possible. We remove features with little useful variation (mostly missing, or heavily imbalanced) and features that are highly correlated for obvious reasons (e.g. total square feet and other component square feet). These should help to reduce the likelihood of overfitting and help improve interpretability. \n",
    "\n",
    "In the 'bottom up' case, we use various algorithmic feature selection techniques (e.g. random forest feature importance) and choose the top 10. This helps us construct the most parsimonious feature set to compare our more complete feature set against. \n",
    "\n",
    "In both cases, we also make several variable transformations - imputing missing features, or one-hot-encoding categorical features, for instance. They are listed below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove features with very little variation and those that are missing for most observations\n",
    "\n",
    "Based on this heatmap of missing variables we see that Alley, PoolQC, Fence and MiscFeature are missing for most values in the dataset. This makes sense, as these are more esoteric features of the house. In the same vein, there is also very little variation in Street, Utilities, LowQualFinSF and MiscVal. For example, most of these houses have all the utilities except for one. While this may ultimately be an important predictor, we believe that this is more likely to overfit and not generalize (after all this observation can only be in validation or train) for any given fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(original_data.isnull(), cbar=False)\n",
    "data = data.drop(columns=['Alley', 'PoolQC', 'Fence', 'MiscFeature'])\n",
    "\n",
    "# Drop variables with very imblanced categories or mostly missing\n",
    "data = data.drop(columns = ['Street', 'Utilities', 'LowQualFinSF', 'MiscVal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove features that are redundant given other variables\n",
    "\n",
    "We then remove aggregated variables that are very close to the sum of its component parts. For instance, TotalBsmtSF is the approximately sum of the BsmtFinSF1, BSmtUnfSF and BsmtFinSF2. Likewise with GrLivArea. There are some minor discrepancies, but I assume that those are immaterial for our analysis and not large/frequent enough to matter. \n",
    "\n",
    "GarageArea / GarageCars is an interesting case, as these two variables capture a similar concept. Indeed, the two variables are highly correlated (see chart below). We keep both in for now, but may decide to remove the 'less informative' one later (e.g. GarageCars). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "### BASEMENTSF ###\n",
    "# Drop the aggregate BsmtSF variable\n",
    "test = pd.DataFrame()\n",
    "test['DiffBsmtSF'] = original_data[\"TotalBsmtSF\"] - original_data['BsmtFinSF1'] - original_data['BsmtUnfSF'] - original_data['BsmtFinSF2']\n",
    "test['DiffBsmtSF'].value_counts()\n",
    "\n",
    "# Remove variable\n",
    "data = data.drop(columns=['TotalBsmtSF'])\n",
    "\n",
    "##################\n",
    "### GRLIVAREA ###\n",
    "# Is GrLivArea simply the sum of 1st and 2nd?\n",
    "# Most of hte time it is - there are 5 observations that are not, but doesn't seem like a goood reason to keep\n",
    "test = pd.DataFrame()\n",
    "test['GrLivArea'] = original_data[\"GrLivArea\"]\n",
    "test['1stAnd2nd'] = original_data[\"1stFlrSF\"] + original_data[\"2ndFlrSF\"]\n",
    "test['Diff'] = test['GrLivArea'] - test['1stAnd2nd']\n",
    "test['Diff'].value_counts()\n",
    "\n",
    "###################\n",
    "### GARAGECARS ###\n",
    "data = data.drop(columns = ['GrLivArea'])\n",
    "original_data['GarageCars'].value_counts()\n",
    "plt.scatter(original_data['GarageCars'],original_data['GarageArea'])\n",
    "#data = data.drop(columns=['GarageCars']) # KEEP FOR NOW, but garage area may be better than garage cars\n",
    "\n",
    "###################\n",
    "### DATETIME    ###\n",
    "# Change date to ordinal representation:\n",
    "data['YrMoSold'] = data['YrSold'].astype(int).astype(str) + \"/\" + data['MoSold'].astype(int).astype(str) + \"/1\"\n",
    "data['YrMoSold'] = data['YrMoSold'].apply(lambda x: dt.strptime(x, \"%Y/%m/%d\").date().toordinal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute features\n",
    "\n",
    "We use the median values to impute missing values to control for possible outliers. We also do not impute any missing categories for now (they will receive 0 across all one-hot encoded dummy variables). In a later analysis, we might do more complex imputation of these missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "# Impute all numeric missings (all dummy missings are coded 0)\n",
    "imp = SimpleImputer(missing_values = np.nan, strategy = 'median')\n",
    "data_numeric = imp.fit_transform(data.select_dtypes(exclude='object'))\n",
    "data_numeric = pd.DataFrame(data_numeric, columns = data.select_dtypes(exclude='object').columns)\n",
    "data = pd.concat([data.select_dtypes(include='object'), data_numeric], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine sparse categories in some features\n",
    "\n",
    "Condition and Exterior features have many sparse levels - we group levels that have very few observations to obtain a bigger category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine categories - very thin categories\n",
    "# Condition1\n",
    "data['Condition1'] = np.where(data['Condition1']=='Norm', 'Norm', 'Not_Norm')\n",
    "data['Condition2'] = np.where(data['Condition2']=='Norm', 'Norm', 'Not_Norm')\n",
    "\n",
    "# Exterior1st\n",
    "data['Exterior1st'] = np.where(data['Exterior1st'].isin(['BrkComm', 'Stone', 'AsphShn', 'ImStucc', \"CBlock\", \"WdShing\", \"Stucco\", \"AsbShng\"]), 'Other', data['Exterior1st'])\n",
    "data['Exterior2nd'] = np.where(data['Exterior2nd'].isin(['Brk Cmn', 'Stone', 'AsphShn', 'Other', \"CBlock\", \"WdShing\", \"Stucco\", \"AsbShng\"]), 'Other', data['Exterior1st'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding for Categoricals\n",
    "\n",
    "We use one-hot encoding to encode categorical features and combine the condition and exterior variables into one variable each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Binarize all categorical (one-hot)\n",
    "data = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simplify the Condition Feature\n",
    "data['Condition_Norm'] = data['Condition1_Norm'] + data['Condition2_Norm']\n",
    "data['Condition_Not_Norm'] = data['Condition1_Not_Norm'] + data['Condition2_Not_Norm']\n",
    "data['Condition_Norm'] = np.where(data['Condition_Norm']==2, 1, data['Condition_Norm'])\n",
    "data['Condition_Not_Norm'] = np.where(data['Condition_Not_Norm']==2, 1, data['Condition_Not_Norm'])\n",
    "\n",
    "# Simplify the Exterior Feature\n",
    "data['Exterior_VinylSd'] = data['Exterior1st_VinylSd'] + data['Exterior2nd_VinylSd']\n",
    "data['Exterior_HdBoard'] = data['Exterior1st_HdBoard'] + data['Exterior2nd_HdBoard']\n",
    "data['Exterior_MetalSd'] = data['Exterior1st_MetalSd'] + data['Exterior2nd_MetalSd']\n",
    "data['Exterior_Wd Sdng'] = data['Exterior1st_Wd Sdng'] + data['Exterior2nd_Wd Sdng']\n",
    "data['Exterior_Plywood'] = data['Exterior1st_Plywood'] + data['Exterior2nd_Plywood']\n",
    "data['Exterior_Other']   = data['Exterior1st_Other']   + data['Exterior2nd_Other']\n",
    "data['Exterior_CemntBd'] = data['Exterior1st_CemntBd'] + data['Exterior2nd_CemntBd']\n",
    "data['Exterior_BrkFace'] = data['Exterior1st_BrkFace'] + data['Exterior2nd_BrkFace']\n",
    "\n",
    "data['Exterior_VinylSd'] = np.where(data['Exterior_VinylSd'] == 2, 1, data['Exterior_VinylSd'])\n",
    "data['Exterior_HdBoard'] = np.where(data['Exterior_HdBoard'] == 2, 1, data['Exterior_HdBoard'])\n",
    "data['Exterior_MetalSd'] = np.where(data['Exterior_MetalSd'] == 2, 1, data['Exterior_MetalSd'])\n",
    "data['Exterior_Wd Sdng'] = np.where(data['Exterior_Wd Sdng'] == 2, 1, data['Exterior_Wd Sdng'])\n",
    "data['Exterior_Plywood'] = np.where(data['Exterior_Plywood'] == 2, 1, data['Exterior_Plywood'])\n",
    "data['Exterior_Other'] = np.where(data['Exterior_Other'] == 2, 1, data['Exterior_Other'])\n",
    "data['Exterior_CemntBd'] = np.where(data['Exterior_CemntBd'] == 2, 1, data['Exterior_CemntBd'])\n",
    "data['Exterior_BrkFace'] = np.where(data['Exterior_BrkFace'] == 2, 1, data['Exterior_BrkFace'])\n",
    "\n",
    "cols_to_drop = list(filter(lambda x: re.search(r'Condition1|Condition2|Exterior1|Exterior2', x), data.columns))\n",
    "data = data.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert features to the right type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['OverallQual'] = original_data['OverallQual'].astype('category')\n",
    "data['OverallCond'] = original_data['OverallCond'].astype('category', ordered=True)\n",
    "data['MSSubClass'] = original_data['MSSubClass'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic feature importance to sense check\n",
    "\n",
    "We do two types of feature importance checks:\n",
    "- First we investigate the pairwise correlation between all variables and identify any set variables that are highly correlated. This led us to discover the redundant sets of variables discussed above.\n",
    "- Second we use the Random Forest feature importance algorithm to identify the variables that lead to the largest information gain. This mostly makes sense and we use this to construct the 'bottom up' dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(20,10))\n",
    "corr = original_data.select_dtypes(exclude='object').corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .7})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use an algorithmic Feature Selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_variables = ['SalePrice']\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(data.drop(columns=['SalePrice']), data['SalePrice'])\n",
    "rf.feature_importances_\n",
    "\n",
    "feature_importances = pd.DataFrame(rf.feature_importances_.round(4),\n",
    "                                   index = data.drop(columns=['SalePrice']).columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "feature_importances[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Simple Version with fewer variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create a really simple version with 10 variables\n",
    "most_impt_cols = list(feature_importances[:10].index)\n",
    "most_impt_cols.append('SalePrice')\n",
    "\n",
    "def get_simple_data(input_data, colnames):\n",
    "    output_data = input_data[colnames]\n",
    "    return(output_data)\n",
    "    \n",
    "simple_data = get_simple_data(data, most_impt_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Baseline & Early Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with a simple 15% dev set, but we have found that for this amount of data, the differences in the models and their scores on the dev sets can vary significantly based on which rows end up in the train and dev sets. Repeated random sub-sampling cross validation helps us get more consistent results.\n",
    "\n",
    "Note that we do not split out the dev data using the most recent years, which would be the proper way to create a dev set if our task were explicitly to predict future home prices. The test data appears to have rows from all the years represented in the train set, so we built dev sets that sample from across the train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# still to do : choose one version of pandas to use so that our code all agrees\n",
    "# and I don't have to read in a new dataset here \n",
    "data['LogSalePrice'] = np.log(data['SalePrice'])\n",
    "NUM_CROSS_VALS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the list of different cross val splits\n",
    "cross_val_list = []\n",
    "for i in range(NUM_CROSS_VALS):\n",
    "    split_idx = int(data.shape[0] * .85)\n",
    "    # line below is what shuffles\n",
    "    data = data.sample(frac=1)\n",
    "    train_df = data[:split_idx]\n",
    "    dev_df = data[split_idx:]\n",
    "    split_dict = {'train_df': train_df,\n",
    "                  'dev_df': dev_df}\n",
    "    cross_val_list.append(split_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our primary error metric, we focus on the root mean squared error of the logarithm of the prices, which is the error metric being used to create the leaderboard for this kaggle competition. See rmsle() in shared_functions.py for our implementation of the root mean squared error, an implementation we found from Mark Nagelberg on Kaggle: https://www.kaggle.com/marknagelberg/rmsle-function.\n",
    "\n",
    "When we consulted our resident real estate expert, Hilary's dad, about this problem, he told us that only one of these factors matters - \"location, location, location.\" In the spirit of that insight, we created a baseline \"model\" which looks at what neighborhood the house is in and takes the mean price of houses from that neighborhood in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: figure out why i'm getting nans now\n",
    "# when i wasn't in original notebook\n",
    "def baseline_pred(row,\n",
    "                  train_df):\n",
    "    for col in train_df:\n",
    "        if 'Neighborhood' in col:\n",
    "            if row[col] == 1:\n",
    "                neighborhood_var = col\n",
    "                break\n",
    "    return np.nanmean(train_df[train_df[neighborhood_var]==1]['LogSalePrice'])\n",
    "\n",
    "def get_baseline_cross_val(cross_val_list):\n",
    "    all_rmses = []\n",
    "    for di in cross_val_list:\n",
    "        dev_df = di['dev_df']\n",
    "        dev_df['baseline_pred'] = dev_df.apply(lambda row: baseline_pred(row,\n",
    "                                                                         di['train_df']), axis=1)\n",
    "        rmse = sf.rmsle(list(np.exp(dev_df['LogSalePrice'])), list(np.exp(dev_df['baseline_pred'])))\n",
    "        all_rmses.append(rmse)\n",
    "    return np.nanmean(all_rmses) \n",
    "\n",
    "# baseline RMSLE\n",
    "print(\"Baseline RMSLE: {:.3f}\".format(get_baseline_cross_val(cross_val_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this as a baseline, we began exploring how different types of models perform on the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "\n",
    "We begin with linear regression as the standard choice for a regression problem. In ordinary least squares regression, the regression line is fit by minimizing the sum of squared residuals between the predicted line and the true data points. We can interpret the resulting coefficients on each feature as representing the additional impact of a one-unit change in that feature on the final price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_param_list = {LinearRegression: [{}]}\n",
    "outcome_vars = ['LogSalePrice', 'SalePrice']\n",
    "# for all models, we'll try with both the full feature set and the \"top 10\" feature set\n",
    "feature_sets = [[col for col in data.columns if col not in ['YrMoSold', 'LogSalePrice', 'SalePrice']],\n",
    "               [col for col in data.columns if col in simple_data.columns and col not in ['SalePrice', 'LogSalePrice']]]\n",
    "lrdf = sf.try_different_models(cross_val_list, \n",
    "                               models_to_param_list,\n",
    "                               outcome_vars, \n",
    "                               feature_sets)\n",
    "lrdf.sort_values('Root MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree-based regressors\n",
    "\n",
    "The family of tree-based regressors learns a series of simple decision rules to predict the final sale price. The decision tree regressor makes one single Decision Tree, whereas the Random Forest regressor trains an ensemble of decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models_to_param_list = {DecisionTreeRegressor: [{}], \n",
    "                        RandomForestRegressor: [{'n_estimators': 10},\n",
    "                                                {'max_features': 50, 'min_samples_leaf': 3, 'n_estimators': 20},\n",
    "                                                {'min_samples_leaf': 3, 'n_estimators': 20}]}\n",
    "\n",
    "df = sf.try_different_models(cross_val_list, \n",
    "                             models_to_param_list,\n",
    "                             outcome_vars, \n",
    "                             feature_sets)\n",
    "df.sort_values('Root MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest regressor shows up in the table as the DecisionTreeRegressor with the parentheses around the whole function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_param_list = {BayesianRidge: [{}], \n",
    "                        KNeighborsRegressor: [{}]}\n",
    "\n",
    "gb_df = sf.try_different_models(cross_val_list, \n",
    "                             models_to_param_list,\n",
    "                             outcome_vars, \n",
    "                             feature_sets)\n",
    "gb_df.sort_values('Root MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## experimenting with voting methods\n",
    "\n",
    "lin_model = lrdf.iloc[0]['Model'][0]\n",
    "lin2_model = lrdf.iloc[1]['Model'][0]\n",
    "lin_features = lrdf.iloc[0]['Features']\n",
    "lin2_features = lrdf.iloc[1]['Features']\n",
    "rf_model = df.iloc[8]['Model'][0]\n",
    "rf_features = df.iloc[8]['Features']\n",
    "br_model = gb_df.iloc[0]['Model'][0]\n",
    "br_features = gb_df.iloc[0]['Features']\n",
    "        \n",
    "### TODO: something isn't working right - predictions are like, sticking in memory somewhere\n",
    "test_rmsle = sf.predict_via_vote_cross_validated(cross_val_list,\n",
    "                                                 [lin_model, rf_model], \n",
    "                                                 [lin_features, rf_features],\n",
    "                                                 'LogSalePrice')\n",
    "test_rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## train the two models on all the data\n",
    "final_train_data = data.filter([col for col in data.columns if col not in ['YrMoSold', 'LogSalePrice', 'SalePrice']])\n",
    "test_data = pd.read_csv('clean_test_data.csv')\n",
    "final_test_data = test_data.filter([col for col in data.columns if col not in ['YrMoSold', 'LogSalePrice', 'SalePrice']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in final_train_data.columns:\n",
    "    if col not in final_test_data.columns:\n",
    "        final_test_data[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lin = LinearRegression()\n",
    "final_lin.fit(final_train_data,\n",
    "              data['LogSalePrice'])\n",
    "\n",
    "final_rf = RandomForestRegressor(max_features = 50, \n",
    "                                  min_samples_leaf = 3, \n",
    "                                  n_estimators = 20)\n",
    "final_rf.fit(final_train_data,\n",
    "             data['LogSalePrice'])\n",
    "\n",
    "## make the predictions on the kaggle test set\n",
    "lin_preds = final_lin.predict(final_test_data)\n",
    "rf_preds = final_rf.predict(final_test_data)\n",
    "\n",
    "print(lin_preds[:5])\n",
    "print(rf_preds[:5])\n",
    "\n",
    "avg_preds = []\n",
    "for i in range(len(lin_preds)):\n",
    "    avg_preds.append((lin_preds[i] + rf_preds[i]) / 2)\n",
    "\n",
    "## exponentiate them, make them the right format\n",
    "transformed_preds = np.exp(avg_preds)\n",
    "\n",
    "ids = final_test_data['Id'].values\n",
    "submission_df = pd.DataFrame(data={'Id': ids, 'SalePrice': transformed_preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('temptemp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where this modeling section is going\n",
    "\n",
    "In our final report, we intend to include more sections of different typess of models and ensembles, and to build out the sections with some deeper explanations.\n",
    "\n",
    "Some things we intend to try:\n",
    "- Bayesian ridge models\n",
    "- K nearest neighbor\n",
    "- Boosting ensembles\n",
    "- Bagging ensembles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "In this section, we'll go into more detail about how we actually iterated on models and chose whichever ones we end up deciding are our best. Our primary tools will be this error correlation table, which we'll use to look at patterns of errors the model is making, and diagnostics to determine whether or not the model is overfitting. We'll compare different models to each other and explain the model or ensemble that gives us the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this still only works on individual models, it doens't average the correlations over a set of models\n",
    "# this tool is really more exploratory than anything - look at a couple models from the set you care about\n",
    "# and see what the trends are\n",
    "\n",
    "# use this variable to specify which model specification to use\n",
    "df_and_row_to_use = lrdf.iloc[0]\n",
    "# use this variable to specify which in the list of models trained with that specification to use\n",
    "model_to_use = df_and_row_to_use['Model'][1]\n",
    "# don't change this\n",
    "features_to_use = df_and_row_to_use['Features']\n",
    "\n",
    "def create_error_correlation_table(model,\n",
    "                                   outcome_var,\n",
    "                                   feature_set,\n",
    "                                   dev_df):\n",
    "    \n",
    "    '''\n",
    "    finds correlation between absolute value of error\n",
    "    and each feature\n",
    "    '''\n",
    "    \n",
    "    final_data = {'col': feature_set}\n",
    "    dev_df = dev_df.reset_index()\n",
    "    \n",
    "    dev_preds = model.predict(dev_df[feature_set])\n",
    "\n",
    "    rmsles = []\n",
    "    for i in range(len(dev_preds)):\n",
    "        rmsles.append(sf.rmsle([dev_df[outcome_var][i]], [dev_preds[i]]))\n",
    "\n",
    "    plt.clf()\n",
    "    plt.hist(rmsles, bins=20)\n",
    "    plt.xlabel(\"RMSLE\")\n",
    "    plt.ylabel(\"Number of Occurrences\")\n",
    "    plt.show()\n",
    "\n",
    "    dev_df['linear_reg_errors'] = rmsles\n",
    "\n",
    "    corrs = []\n",
    "    for col in feature_set:\n",
    "        try:\n",
    "            cor = np.corrcoef(abs(dev_df['linear_reg_errors']), dev_df[col])[0,1]\n",
    "            corrs.append(cor)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    final_data['correlation'] = corrs \n",
    "    \n",
    "    corrs_df = pd.DataFrame(data=final_data)\n",
    "    corrs_df = corrs_df.dropna()\n",
    "    return corrs_df\n",
    "  \n",
    "# table for our LR with all the features\n",
    "corrs_df = create_error_correlation_table(model_to_use, 'LogSalePrice', features_to_use, dev_df)\n",
    "corrs_df.reindex(corrs_df.correlation.abs().sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper Dive into Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This problem is asking us to predict the sale prices of homes that are randomly distributed within the dataset in terms of time. It is not asking to predict the sales prices for homes that will be sold in the future. In this way, the problem is not actually a useful real-world problem to solve. It isn't that useful to predict the sales prices of homes that have already been sold; a useful model would predict the prices of homes that will be sold in the future. In order to create a model that would make these future home price predictions, we would possibly need to exclude or weight differently the more recent data in our training set because we would want the recent data to play a bigger role in training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways that the problem posed based on this data could be reformulated to make it more interesting or useful to stake-holders. To consider these, we break the context down into two different cases. First, we can consider the data and the problem from the perspective of a prospective home buyer. Next, we can consider the data and the problem from the perspective of a prospective home seller. There are different stake-holders within these groups as well, such as real estate agents or real estate developers, who might have different levels of interest or fluency with different features or models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key question for any home buyer is: What is the right price to offer for a given home? To answer this question, people spend a lot of time considering \"comps\" which are comparable properties or homes that they could use to estimate the market value of a home. An interesting reformulation of this problem would be to consider a model that uses clustering and filtering so that homes are only compared to similar homes based on key factors, such as the neighborhood, the location within the neighborhood (i.e. corner lot or on a quiet street), the size of the home and the year it was built. Typically, a home buyer would have already narrowed down their homes by neighborhood and other key factors, so a model that predicted prices within comparable properties would be most useful to prospective buyers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key question for any home seller is: What is the right price to ask for this home? Setting the asking price is more of an art than a science, but machine learning could change that. Sellers would not only want to know about what price a home will ultimately sell for, but also how to set the asking price to maximize this value. In order to answer this question, the model would not only need to be changed in order to predict future sale prices, but the data set would need to also include the asking prices of homes, so that the model could include information about the relationship between the asking price and the final sale price. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There would be several key variables to be added to the dataset that would make these questions more answerable. First, more specific time information about home sales would be helpful. The current dataset only offers information to the month, not the day. Second, more specific information about the location would be helpful--we don't know which houses are on corner lots, or busy streets or quiet cul-de-sacs. This additional feature would allow for more analysis of the quality of the home's location. To make the location analysis most accurate, we would ideally have latitude and longitude data about the exact location of each home.\n",
    "\n",
    "Finally, more information about larger economic trends would be helpful. While it is impossible to predict the future, trends in other economic indicators could have predictive value for future home prices. A model that was able to accurately predict future home prices would need to evaluate the predictive value of various other economic indicators.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
