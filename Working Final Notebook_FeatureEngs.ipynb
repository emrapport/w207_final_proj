{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Baseline\n",
    "Yang Wei Neo, Emily Rapport, Hilary Yamtich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "from rfpimp import *\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# note: this notebook requires pandas 0.21.0 or newer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from datetime import datetime as dt\n",
    "import re as re\n",
    "import pickle as pk\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# For producing decision tree diagrams.\n",
    "from IPython.core.display import Image, display\n",
    "from sklearn.externals.six import StringIO\n",
    "\n",
    "from dateutil import parser\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic EDA\n",
    "\n",
    "# The training data needs to be saved in the same location.\n",
    "original_data = pd.read_csv('train.csv')\n",
    "data = original_data.copy()\n",
    "\n",
    "# We also read in kaggle testing data so that we can run same feature eng steps\n",
    "original_test_data = pd.read_csv('test.csv')\n",
    "test_data = original_test_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background to the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset and Framing the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this problem is to use the Ames Housing Dataset to create a model to predict the sale prices of homes randomly selected from within the original dataset. \n",
    "\n",
    "The Ames Housing Dataset was compiled by Dean De Cock in 2011. It is based on all the housing sales that occured in Ames, Iowa during the period from 2006 to 2010. It was created as an alternative to the Boston Housing Dataset and it includes more feature variables (79) that are related to housing price. The original dataset included 2,390 observations. The goal of the creator was to include a wider set of explanatory variables than the original Boston Housing Dataset to allow for more advanced regression techniques to be applied.\n",
    "\n",
    "The problem itself is based on Kaggle competition to use advanced regression techniques to create a model to predict the sale price of a home. The test data is randomly selected from within the original Ames Housing Dataset. The training dataset used in the Kaggle competition includes 1,460 observations. The goal is to accurately predict the sale prices of the homes in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Sale Price Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we look at the distribution of the sale price variable using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(\"SalePrice\", bins = 50)\n",
    "plt.xlabel(\"Sale Prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"LogSalePrice\"] = np.log(data[\"SalePrice\"])\n",
    "data.hist(\"LogSalePrice\", bins = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a more normally distributed set of data. This makes our highly skewed distribution less skewed and allows our sale price data to meet the assumptions of inferential statistics and linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "We have two broad approaches to feature engineering - 'top down' and 'bottom up'. \n",
    "\n",
    "In the 'top down' case, we start with all the variables and work to eliminate / simplify variables as much as possible. We remove features with little useful variation (mostly missing, or heavily imbalanced) and features that are highly correlated for obvious reasons (e.g. total square feet and other component square feet). These should help to reduce the likelihood of overfitting and help improve interpretability. \n",
    "\n",
    "In the 'bottom up' case, we use various algorithmic feature selection techniques (e.g. random forest feature importance) and choose the top 10. This helps us construct the most parsimonious feature set to compare our more complete feature set against. \n",
    "\n",
    "In both cases, we also make several variable transformations - imputing missing features, or one-hot-encoding categorical features, for instance. They are listed below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove features with very little variation and those that are missing for most observations\n",
    "\n",
    "Based on this heatmap of missing variables we see that Alley, PoolQC, Fence and MiscFeature are missing for most values in the dataset. This makes sense, as these are more esoteric features of the house. In the same vein, there is also very little variation in Street, Utilities, LowQualFinSF and MiscVal. For example, most of these houses have all the utilities except for one. While this may ultimately be an important predictor, we believe that this is more likely to overfit and not generalize (after all this observation can only be in validation or train) for any given fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(original_data.isnull(), cbar=False)\n",
    "\n",
    "def remove_low_var_and_missin_features(inp_data):\n",
    "    inp_data = inp_data.drop(columns=['Alley', 'PoolQC', 'Fence', 'MiscFeature'])\n",
    "\n",
    "    # Drop variables with very imblanced categories or mostly missing\n",
    "    inp_data = inp_data.drop(columns = ['Street', 'Utilities', 'LowQualFinSF', 'MiscVal'])\n",
    "    \n",
    "    return inp_data\n",
    "\n",
    "data = remove_low_var_and_missin_features(data)\n",
    "test_data = remove_low_var_and_missin_features(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove features that are redundant given other variables\n",
    "\n",
    "We then remove aggregated variables that are very close to the sum of its component parts. For instance, TotalBsmtSF is the approximately sum of the BsmtFinSF1, BSmtUnfSF and BsmtFinSF2. Likewise with GrLivArea. There are some minor discrepancies, but I assume that those are immaterial for our analysis and not large/frequent enough to matter. \n",
    "\n",
    "GarageArea / GarageCars is an interesting case, as these two variables capture a similar concept. Indeed, the two variables are highly correlated (see chart below). We keep both in for now, but may decide to remove the 'less informative' one later (e.g. GarageCars). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_features(inp_data):\n",
    "    ##################\n",
    "    ### BASEMENTSF ###\n",
    "    # Drop the aggregate BsmtSF variable\n",
    "    test = pd.DataFrame()\n",
    "    test['DiffBsmtSF'] = original_data[\"TotalBsmtSF\"] - original_data['BsmtFinSF1'] - original_data['BsmtUnfSF'] - original_data['BsmtFinSF2']\n",
    "    test['DiffBsmtSF'].value_counts()\n",
    "\n",
    "    # Remove variable\n",
    "    inp_data = inp_data.drop(columns=['TotalBsmtSF'])\n",
    "\n",
    "    ##################\n",
    "    ### GRLIVAREA ###\n",
    "    # Is GrLivArea simply the sum of 1st and 2nd?\n",
    "    # Most of hte time it is - there are 5 observations that are not, but doesn't seem like a goood reason to keep\n",
    "    test = pd.DataFrame()\n",
    "    test['GrLivArea'] = original_data[\"GrLivArea\"]\n",
    "    test['1stAnd2nd'] = original_data[\"1stFlrSF\"] + original_data[\"2ndFlrSF\"]\n",
    "    test['Diff'] = test['GrLivArea'] - test['1stAnd2nd']\n",
    "    test['Diff'].value_counts()\n",
    "\n",
    "    ###################\n",
    "    ### GARAGECARS ###\n",
    "    inp_data = inp_data.drop(columns = ['GrLivArea'])\n",
    "    original_data['GarageCars'].value_counts()\n",
    "    plt.scatter(original_data['GarageCars'],original_data['GarageArea'])\n",
    "    #data = data.drop(columns=['GarageCars']) # KEEP FOR NOW, but garage area may be better than garage cars\n",
    "\n",
    "    ###################\n",
    "    ### DATETIME    ###\n",
    "    # Change date to ordinal representation:\n",
    "    inp_data['YrMoSold'] = inp_data['YrSold'].astype(int).astype(str) + \"/\" + inp_data['MoSold'].astype(int).astype(str) + \"/1\"\n",
    "    inp_data['YrMoSold'] = inp_data['YrMoSold'].apply(lambda x: dt.strptime(x, \"%Y/%m/%d\").date().toordinal())\n",
    "    return inp_data\n",
    "    \n",
    "data = remove_redundant_features(data)\n",
    "test_data = remove_redundant_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save copy of lot frontage - original \n",
    "data_LotFrontage_original = data['LotFrontage'].copy()\n",
    "\n",
    "def impute_missing_vals(inp_data):\n",
    "    # Impute all numeric missings (all dummy missings are coded 0)\n",
    "    imp = SimpleImputer(missing_values = np.nan, strategy = 'median')\n",
    "    data_numeric = imp.fit_transform(inp_data.select_dtypes(exclude='object'))\n",
    "    data_numeric = pd.DataFrame(data_numeric, columns = inp_data.select_dtypes(exclude='object').columns)\n",
    "    inp_data = pd.concat([inp_data.select_dtypes(include='object'), data_numeric], axis = 1)\n",
    "    return inp_data\n",
    "\n",
    "data = impute_missing_vals(data)\n",
    "test_data = impute_missing_vals(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine sparse categories in some features\n",
    "\n",
    "Condition and Exterior features have many sparse levels - we group levels that have very few observations to obtain a bigger category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_thin_categories(inp_data):\n",
    "    # Combine categories - very thin categories\n",
    "    # Condition1\n",
    "    inp_data['Condition1'] = np.where(inp_data['Condition1']=='Norm', 'Norm', 'Not_Norm')\n",
    "    inp_data['Condition2'] = np.where(inp_data['Condition2']=='Norm', 'Norm', 'Not_Norm')\n",
    "\n",
    "    # Exterior1st\n",
    "    inp_data['Exterior1st'] = np.where(inp_data['Exterior1st'].isin(['BrkComm', 'Stone', 'AsphShn', 'ImStucc', \"CBlock\", \"WdShing\", \"Stucco\", \"AsbShng\"]), 'Other', inp_data['Exterior1st'])\n",
    "    inp_data['Exterior2nd'] = np.where(inp_data['Exterior2nd'].isin(['Brk Cmn', 'Stone', 'AsphShn', 'Other', \"CBlock\", \"WdShing\", \"Stucco\", \"AsbShng\"]), 'Other', inp_data['Exterior1st'])\n",
    "\n",
    "    return inp_data\n",
    "\n",
    "data = combine_thin_categories(data)\n",
    "test_data = combine_thin_categories(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding for Categoricals\n",
    "\n",
    "We use one-hot encoding to encode categorical features and combine the condition and exterior variables into one variable each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(inp_data):\n",
    "    ##########################\n",
    "    # Binarize all categorical (one-hot)\n",
    "    inp_data = pd.get_dummies(inp_data)\n",
    "\n",
    "    # Simplify the Condition Feature\n",
    "    inp_data['Condition_Norm'] = inp_data['Condition1_Norm'] + inp_data['Condition2_Norm']\n",
    "    inp_data['Condition_Not_Norm'] = inp_data['Condition1_Not_Norm'] + inp_data['Condition2_Not_Norm']\n",
    "    inp_data['Condition_Norm'] = np.where(inp_data['Condition_Norm']==2, 1, inp_data['Condition_Norm'])\n",
    "    inp_data['Condition_Not_Norm'] = np.where(inp_data['Condition_Not_Norm']==2, 1, inp_data['Condition_Not_Norm'])\n",
    "\n",
    "    # Simplify the Exterior Feature\n",
    "    inp_data['Exterior_VinylSd'] = inp_data['Exterior1st_VinylSd'] + inp_data['Exterior2nd_VinylSd']\n",
    "    inp_data['Exterior_HdBoard'] = inp_data['Exterior1st_HdBoard'] + inp_data['Exterior2nd_HdBoard']\n",
    "    inp_data['Exterior_MetalSd'] = inp_data['Exterior1st_MetalSd'] + inp_data['Exterior2nd_MetalSd']\n",
    "    inp_data['Exterior_Wd Sdng'] = inp_data['Exterior1st_Wd Sdng'] + inp_data['Exterior2nd_Wd Sdng']\n",
    "    inp_data['Exterior_Plywood'] = inp_data['Exterior1st_Plywood'] + inp_data['Exterior2nd_Plywood']\n",
    "    inp_data['Exterior_Other']   = inp_data['Exterior1st_Other']   + inp_data['Exterior2nd_Other']\n",
    "    inp_data['Exterior_CemntBd'] = inp_data['Exterior1st_CemntBd'] + inp_data['Exterior2nd_CemntBd']\n",
    "    inp_data['Exterior_BrkFace'] = inp_data['Exterior1st_BrkFace'] + inp_data['Exterior2nd_BrkFace']\n",
    "\n",
    "    inp_data['Exterior_VinylSd'] = np.where(inp_data['Exterior_VinylSd'] == 2, 1, inp_data['Exterior_VinylSd'])\n",
    "    inp_data['Exterior_HdBoard'] = np.where(inp_data['Exterior_HdBoard'] == 2, 1, inp_data['Exterior_HdBoard'])\n",
    "    inp_data['Exterior_MetalSd'] = np.where(inp_data['Exterior_MetalSd'] == 2, 1, inp_data['Exterior_MetalSd'])\n",
    "    inp_data['Exterior_Wd Sdng'] = np.where(inp_data['Exterior_Wd Sdng'] == 2, 1, inp_data['Exterior_Wd Sdng'])\n",
    "    inp_data['Exterior_Plywood'] = np.where(inp_data['Exterior_Plywood'] == 2, 1, inp_data['Exterior_Plywood'])\n",
    "    inp_data['Exterior_Other'] = np.where(inp_data['Exterior_Other'] == 2, 1, inp_data['Exterior_Other'])\n",
    "    inp_data['Exterior_CemntBd'] = np.where(inp_data['Exterior_CemntBd'] == 2, 1, inp_data['Exterior_CemntBd'])\n",
    "    inp_data['Exterior_BrkFace'] = np.where(inp_data['Exterior_BrkFace'] == 2, 1, inp_data['Exterior_BrkFace'])\n",
    "\n",
    "    cols_to_drop = list(filter(lambda x: re.search(r'Condition1|Condition2|Exterior1|Exterior2', x), inp_data.columns))\n",
    "    inp_data = inp_data.drop(columns=cols_to_drop)\n",
    "    \n",
    "    return inp_data\n",
    "\n",
    "data = one_hot_encode(data)\n",
    "test_data = one_hot_encode(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Test Data to Match Train Data's One-Hot Encoding\n",
    "\n",
    "When we one-hot encode, the number of new columns we create depends on the number of classes contained in each column. For instance, if our data has 10 possible values for MSSubClass, then we'll end up with 10 new columns, one for each of those values. However, when we do the same operation on our test data, we end up with a different set of columns, since the data in the two sets is different. We could end up with features that are only in our train data, or features that are only in our test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# detect features that are only in our train data\n",
    "for column in data.columns:\n",
    "    if column not in test_data.columns:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# detect features that are only in our test data \n",
    "for column in test_data.columns:\n",
    "    if column not in data.columns:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our model trains on features that are present in the train data, it will need to have the same columns available in the test data. Since these columns represent particular values of a categorical variable that just weren't present in our test data, we can simply add the columns to our test data with a 0 for each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    if col not in test_data.columns:\n",
    "        test_data[col] = 0\n",
    "test_data = test_data[[col for col in data.columns if col not in ['LogSalePrice', 'SalePrice']]]\n",
    "\n",
    "## confirm that the only columns that differ between the two sets are now the outcome variables,\n",
    "## which our test data should not have\n",
    "\n",
    "print([column for column in data.columns if column not in test_data.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A critical final step: re-order the columns in the test data frame so that they're in the same order as the train data frame. Otherwise, scikit-learn will appear to be predicting correctly, but will actually be looking at the wrong variables and make bad predictions (this actually happened to us for a few days). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_col_order = [col for col in data.columns if col not in ['SalePrice', 'LogSalePrice']]\n",
    "test_data = test_data[train_col_order]\n",
    "\n",
    "## confirm that columns are now in same order\n",
    "## we should see an array of \"trues\" \n",
    "train_col_order == test_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert features to the right type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feature_types(inp_data, original):\n",
    "    inp_data['OverallQual'] = original['OverallQual'].astype('category')\n",
    "    inp_data['OverallCond'] = original['OverallCond'].astype('category', ordered=True)\n",
    "    inp_data['MSSubClass'] = original['MSSubClass'].astype('category')\n",
    "    return inp_data\n",
    "\n",
    "data = convert_feature_types(data, original_data)\n",
    "test_data = convert_feature_types(test_data, original_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute features\n",
    "\n",
    "- There are several variables that are missing substantial amount of data; however, one of them is especially critical for predicting home prices - the Lot Frontage. We use a kNN to impute the missing lot frontages - a cursory look at the distribution shows that the values follow a tighter, but normal-ish distribution. This is similar to the distribution of the nonmissing data. \n",
    "\n",
    "- Not shown here is a series of tests we ran to compare the accuracy when using the median imputation vs. the more precise kNN imputation. The results are very close (difference is to the fourth decimal place). In linear regression, the kNN imputation does marginally better regardless of the regularization used. In Random Forest - the model with kNN imputed values actually does worse for iterations with simpler trees and better for iterations with more complex trees. We suspect that this might be because more complex trees may be more likely to overfit imputed values.\n",
    "\n",
    "- Regardless, we think the evidence points marginally in favor of using the imputation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: YW - can you turn this into a function and handle train/test however you think is appropriate?\n",
    "# Impute one or two key variables that are missing a substantial amount of data\n",
    "# Normalize data before running \n",
    "scaler = StandardScaler(copy=True)\n",
    "data_scaled = scaler.fit_transform(data.values)\n",
    "data_scaled = pd.DataFrame(data_scaled, index = data.index, columns = data.columns)\n",
    "\n",
    "# For example, Lot Frontage\n",
    "knn = KNeighborsRegressor(n_neighbors=5, weights = 'distance')\n",
    "\n",
    "# Isolate training data and missing data\n",
    "notmissing_flag = data_LotFrontage_original.notnull()\n",
    "missing_flag = data_LotFrontage_original.isnull()\n",
    "\n",
    "# Define test data\n",
    "lotfrontage_data = data_scaled[notmissing_flag].drop(columns='LotFrontage')\n",
    "lotfrontage_labels = data_LotFrontage_original[notmissing_flag]\n",
    "\n",
    "# test missing (prediction data)\n",
    "lotfrontage_testdata = data_scaled.drop(columns='LotFrontage')\n",
    "\n",
    "# Use KNN to predict missing data\n",
    "knn.fit(lotfrontage_data, lotfrontage_labels)\n",
    "lotpredictions = knn.predict(lotfrontage_testdata).astype(int)\n",
    "\n",
    "# Plug it back into training data\n",
    "data_LotFrontage_imputed = data_LotFrontage_original.copy()\n",
    "data_LotFrontage_imputed[missing_flag] = lotpredictions\n",
    "data_LotF_df = pd.DataFrame(np.column_stack((data_LotFrontage_imputed, missing_flag)), columns = ['LotFrontage_Imputed', 'Missing'])\n",
    "\n",
    "# Plot the two histograms:\n",
    "dataforplot = data_LotF_df.groupby('Missing')['LotFrontage_Imputed']\n",
    "dataforplot.plot(kind='hist', bins = 30, density = True, histtype = 'step')\n",
    "\n",
    "# Plug back into the data\n",
    "data_LotFrontage_imputed_df = pd.DataFrame({'LotFrontage_Imputed': data_LotFrontage_imputed})\n",
    "data = pd.concat([data.drop(columns='LotFrontage'), data_LotFrontage_imputed_df], axis = 1)\n",
    "\n",
    "## todo: this is just a hack for now so that they'll match - fix this after we decide how to handle all this\n",
    "test_data['LotFrontage_Imputed'] = test_data['LotFrontage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_new_features(inp_data):\n",
    "    inp_data['Basement'] = np.where(inp_data['BsmtFinSF1'] + inp_data['BsmtFinSF2'] + inp_data['BsmtUnfSF'] > 1, 1, 0)\n",
    "    inp_data['WoodDeck'] = np.where(inp_data['WoodDeckSF'] > 0, 1, 0)\n",
    "    return inp_data\n",
    "\n",
    "data = add_new_features(data)\n",
    "train_data = add_new_features(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "First we tried PCA without scaling and found that the first Principal Component captured 99% of the variation! This seemed too good to be true - indeed it was as our underlying data was not normalized or standardized. So most of the variation came from variables that were very large in units (e.g. LotSize). We validated this and saw that the units of the PCA itself were uncannily similar to LotSize; and that the principal components were also highly correlated with the LotSize variable.\n",
    "\n",
    "After normalizing the underlying data we found that the PCA made more sense: even 50 components only explained 75% of the variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo YW: can you handle train/test however you think is appropriate here?\n",
    "# Initialize parameters \n",
    "num_components = 50\n",
    "data_for_pca = data.drop(columns=['YrMoSold', 'SalePrice', 'Id', 'LogSalePrice'])\n",
    "pca_names = ['pca_' + str(i) for i in range(num_components)]\n",
    "\n",
    "# First scale\n",
    "scaler = StandardScaler(copy=True)\n",
    "data_scaled = scaler.fit_transform(data_for_pca.values)\n",
    "data_scaled = pd.DataFrame(data_scaled, index = data_for_pca.index, columns = data_for_pca.columns)\n",
    "\n",
    "# Fit PCAe\n",
    "pca = PCA(n_components=num_components)\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "data_pca = pd.DataFrame(data_pca, columns=pca_names)\n",
    "\n",
    "# Show Results\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel('Num Components')\n",
    "plt.ylabel('Incremental Explained Variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cbind to dataframe\n",
    "data = pd.concat([data.reset_index(drop=True), data_pca], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Interactions\n",
    "\n",
    "- First we investigate the pairwise correlation between all variables and identify any set variables that are highly correlated. This led us to discover the redundant sets of variables discussed above.\n",
    "- Further, we examine what variables seem to drive most of the variation in the dataset (by looking at those that are highly correlated with the top 5 principal components). Based on the PCA we might surmise that most of the variation in homes is associated with the number/types of rooms and the size of home; less so the meta-attributes (e.g. year sold). And, as expected, the principal components are uncorrelated with each other, by construction. \n",
    "    - PCA_0 seems to be capturing most of the variation that is linearly correlated with SalePrice (even though saleprice and its derivatives were not used in the generation of the PCA)\n",
    "    - PCA_1 seems to be capturing variation associated with the discrete features in the home (e.g. number of baths, rooms, kitchen etc.)\n",
    "    - PCA_2 seems to be capturing variation associated with the size of the home (SF, Frontage, LotArea, etc.)\n",
    "    \n",
    "- In testing (not shown here), PCA with the top 50 components does not improve accuracy (it is actually worse!) It is betwen 1-2 points worse for linear regression and for random forest on various combinations. We decide to exclude the use of the principal components for modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_visual = original_data.select_dtypes(exclude='object')\n",
    "data_for_visual = pd.concat((data_for_visual, data_pca.iloc[:,0:5]), axis = 1)\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(20,10))\n",
    "corr = data_for_visual.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .7})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmic Feature Selection method\n",
    "- We use this to sense check the range of important features. We find several interesting observations:\n",
    "    - Principal components are not necessarily the most predictive of the target variable - other variables, such as size-related variables still rank higher than several of the top 5 principal components - See below. This makes sense because PCA only tries to create components that maximize the variance of the feature vectors and does not consider the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove target variables\n",
    "remove_variables = ['SalePrice', 'LogSalePrice']\n",
    "\n",
    "# Use random forest to remove feature vectors\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(data.drop(columns=remove_variables), data['SalePrice'])\n",
    "\n",
    "# Set up data frame to show feature importance including PCA variables\n",
    "feature_importances = pd.DataFrame(rf.feature_importances_.round(4),\n",
    "                                   index = data.drop(columns=remove_variables).columns, \n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importances[0:10])\n",
    "\n",
    "# Set up data frame to show feature importance excluding PCA variables\n",
    "remove_variables_pca = ['SalePrice', 'LogSalePrice', *pca_names]\n",
    "\n",
    "rf.fit(data.drop(columns=remove_variables_pca), data['SalePrice'])\n",
    "feature_importances_wopca = pd.DataFrame(rf.feature_importances_.round(4),\n",
    "                                   index = data.drop(columns=remove_variables_pca).columns, \n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importances_wopca[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Final data to CSV file\n",
    "data.to_csv('clean_data.csv')\n",
    "\n",
    "# Final data to pickle file\n",
    "outfile = open('clean_data_pickle','wb')\n",
    "pk.dump(data, outfile)\n",
    "outfile.close()\n",
    "\n",
    "# final test data to csv and pickle files\n",
    "test_data.to_csv('clean_test_data.csv')\n",
    "with open('clean_test_data.pkl', 'wb') as outfile:\n",
    "    pk.dump(test_data, outfile)\n",
    "\n",
    "#saving this off for use in next notebook\n",
    "feature_importances_wopca.to_csv('feature_importance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
