{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Baseline\n",
    "Yang Wei Neo, Emily Rapport, Hilary Yamtich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "from rfpimp import *\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "# note: this notebook requires pandas 0.21.0 or newer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import testing_utils as testing\n",
    "import model_training_utils as model_train\n",
    "import ensemble_model_utils as ensemble\n",
    "import math\n",
    "from datetime import datetime as dt\n",
    "import re as re\n",
    "import pickle as pk\n",
    "\n",
    "# For producing decision tree diagrams.\n",
    "from IPython.core.display import Image, display\n",
    "from sklearn.externals.six import StringIO\n",
    "\n",
    "from dateutil import parser\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the pickle file that contains the clean data and other useful stuff?\n",
    "infile = open('./clean_data_pickle','rb')\n",
    "data = pk.load(infile)\n",
    "infile.close()\n",
    "\n",
    "with open('clean_test_data.pkl', 'rb') as infile:\n",
    "    kaggle_test_data = pk.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "# Run Baseline & Early Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with a simple 15% dev set, but we have found that for this amount of data, the differences in the models and their scores on the dev sets can vary significantly based on which rows end up in the train and dev sets. Repeated random sub-sampling cross validation helps us get more consistent results.\n",
    "\n",
    "Note that we do not split out the dev data using the most recent years, which would be the proper way to create a dev set if our task were explicitly to predict future home prices. The test data appears to have rows from all the years represented in the train set, so we built dev sets that sample from across the train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still to do : choose one version of pandas to use so that our code all agrees\n",
    "# and I don't have to read in a new dataset here \n",
    "NUM_CROSS_VALS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of different cross val splits\n",
    "cross_val_list = []\n",
    "for i in range(NUM_CROSS_VALS):\n",
    "    split_idx = int(data.shape[0] * .85)\n",
    "    # line below is what shuffles\n",
    "    data = data.sample(frac=1)\n",
    "    train_df = data[:split_idx]\n",
    "    dev_df = data[split_idx:]\n",
    "    split_dict = {'train_df': train_df,\n",
    "                  'dev_df': dev_df}\n",
    "    cross_val_list.append(split_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our primary error metric, we focus on the root mean squared error of the logarithm of the prices, which is the error metric being used to create the leaderboard for this kaggle competition. See rmsle() in shared_functions.py for our implementation of the root mean squared error, an implementation we found from Mark Nagelberg on Kaggle: https://www.kaggle.com/marknagelberg/rmsle-function.\n",
    "\n",
    "When we consulted our resident real estate expert, Hilary's dad, about this problem, he told us that only one of these factors matters - \"location, location, location.\" In the spirit of that insight, we created a baseline \"model\" which looks at what neighborhood the house is in and takes the mean price of houses from that neighborhood in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# todo: figure out why i'm getting nans now\n",
    "# when i wasn't in original notebook\n",
    "def baseline_pred(row,\n",
    "                  train_df):\n",
    "    for col in train_df:\n",
    "        if 'Neighborhood' in col:\n",
    "            if row[col] == 1:\n",
    "                neighborhood_var = col\n",
    "                break\n",
    "    return np.nanmean(train_df[train_df[neighborhood_var]==1]['LogSalePrice'])\n",
    "\n",
    "def get_baseline_cross_val(cross_val_list):\n",
    "    all_rmses = []\n",
    "    for di in cross_val_list:\n",
    "        dev_df = di['dev_df']\n",
    "        dev_df['baseline_pred'] = dev_df.apply(lambda row: baseline_pred(row,\n",
    "                                                                         di['train_df']), axis=1)\n",
    "        rmse = testing.rmsle(list(np.exp(dev_df['LogSalePrice'])), list(np.exp(dev_df['baseline_pred'])))\n",
    "        all_rmses.append(rmse)\n",
    "    return np.nanmean(all_rmses) \n",
    "\n",
    "# baseline RMSLE\n",
    "print(\"Baseline RMSLE: {:.3f}\".format(get_baseline_cross_val(cross_val_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this as a baseline, we began exploring how different types of models perform on the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YW SECTION\n",
    "#### Linear Regression\n",
    "\n",
    "We begin with linear regression as the standard choice for a regression problem. In ordinary least squares regression, the regression line is fit by minimizing the sum of squared residuals between the predicted line and the true data points. We can interpret the resulting coefficients on each feature as representing the additional impact of a one-unit change in that feature on the final price.\n",
    "\n",
    "- Will not work well with variables that are highly correlated with each other\n",
    "- Indeed this is why linear regression with PCA seems to do just as well as the original variables; the  loss is not as great for linear regression (only 0.04 loss in RMSLE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up different LR models\n",
    "models_to_param_list = {LinearRegression: [{}], \n",
    "                        Lasso: [{'alpha': 0.005}], # lower value is less regularization\n",
    "                        Ridge: [{'alpha': 2}]} # more effective with more regularization\n",
    "                        #ElasticNet: [{'alpha': 0.1}]} # lower value is less regularization\n",
    "\n",
    "# Outcome \n",
    "outcome_vars = ['LogSalePrice']\n",
    "\n",
    "# Create feature sets\n",
    "feature_sets = [[col for col in data.columns if col not in ['YrMoSold', 'LogSalePrice', 'SalePrice']]]\n",
    "\n",
    "# Output model\n",
    "lr_models = model_train.try_different_models(cross_val_list, \n",
    "                                        models_to_param_list,\n",
    "                                        outcome_vars, \n",
    "                                        feature_sets)\n",
    "\n",
    "lr_models.sort_values('Root MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Illustration\n",
    "\n",
    "Bagging, or bootstrap aggregation, is intended to reduce variance in the test error by averaging predictions over very specialized models. While each of these models in isolation is likely to overfit, the ensemble of specialized models ends up being very effective at reducing overall test error. To stress test this assumption, we run several random forest ensembles on models that are increasingly less likely to *individually* overfit. We find that the more likely each individual model is to overfit (either by enforcing a smaller minimum leaf, or by enforcing a higher split size), the lower the error of the ensemble as a whole. We suspect that this is because a large ensemble paired with high variance/low bias individual models gives the best of both worlds.\n",
    "\n",
    "Note however that this phenomenon is not true when we increase the proportion of features used to split at each node (causing any given individual tree to be more likely to overfit). We aren't sure why... !!!\n",
    "\n",
    "Another interesting phenomenon is that the difference between the training and test error decreases as each individual model within the ensemble gets less complex. This is a sign of increasing bias in the underlying model, which makes sense, since each model is more likely to underfit since it has less underlying complexity. \n",
    "\n",
    "This analysis suggests that we ought to lean towards creating more complex individual trees (low bias); the higher variance that results from this ought be offset by the bootstrap aggregation procedure. We will use a grid search to find the optimal combination of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list of tests:\n",
    "param_list = []\n",
    "\n",
    "# Create list of parameter types\n",
    "for min_leaf_size in range(10):\n",
    "    param_list.append({'min_samples_leaf': min_leaf_size, 'n_estimators': 50})\n",
    "    \n",
    "for feature_prop in range(10):\n",
    "    param_list.append({'max_features': feature_prop/10, 'n_estimators': 50})\n",
    "                       \n",
    "for split_size in range(11):\n",
    "    param_list.append({'min_samples_split': split_size, 'n_estimators': 50})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models to show the impact of bagging\n",
    "### THIS TAKES A LONG TIME TO RUN\n",
    "models_to_param_list = {RandomForestRegressor: param_list}\n",
    "feature_sets = [[col for col in data.columns if col not in ['YrMoSold', 'LogSalePrice', 'SalePrice']]]\n",
    "\n",
    "# Run different random forests\n",
    "df = model_train.try_different_models(cross_val_list, \n",
    "                             models_to_param_list,\n",
    "                             outcome_vars, \n",
    "                             feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "fig, ax = plt.subplots(1,3, sharey='row')\n",
    "fig.tight_layout(pad = 1.5)\n",
    "\n",
    "ax[0].plot(df.iloc[0:9]['Root MSE'])\n",
    "ax[0].plot(df.iloc[0:9]['Train MSE'])\n",
    "ax[0].set_xlabel('Min Leaf Size')\n",
    "\n",
    "ax[1].plot(df.iloc[10:20]['Root MSE'])\n",
    "ax[1].plot(df.iloc[10:20]['Train MSE'])\n",
    "ax[1].set_xlabel('% of Features used to split')\n",
    "\n",
    "ax[2].plot(df.iloc[20:30]['Root MSE'])\n",
    "ax[2].plot(df.iloc[20:30]['Train MSE'])\n",
    "ax[2].set_xlabel('Minimum Split Size')\n",
    "\n",
    "fig.suptitle('Bagging with High Variance Models')\n",
    "# !!! remove the axis tick marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Boosting Illustration\n",
    "\n",
    "In contrast, boosting is a process that reduces bias by refitting the model iteratively on the errors from the previous model. Boosting can turn weak learners into a accurate ensemble - we can see this below by showing how a relatively weak learner (tree of depth 3) actually appears to have the best accuracy in a GBM as compared to a GBM with greater depth. However, GBMs do have a tendency towards high variance (as seen by how quickly the model overfits the training data relative to the test data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list of tests:\n",
    "param_list = []\n",
    "# Create list of parameter types\n",
    "for depth in range(20):\n",
    "    param_list.append({'max_depth': depth, 'n_estimators': 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Gradient Boosting Results\n",
    "models_to_param_list = {GradientBoostingRegressor: param_list}\n",
    "df_boosting = model_train.try_different_models(cross_val_list, \n",
    "                             models_to_param_list,\n",
    "                             outcome_vars, \n",
    "                             feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the DF Boosting Results.\n",
    "plt.plot(df_boosting.index, df_boosting['Root MSE'])\n",
    "plt.plot(df_boosting.index, df_boosting['Train MSE'])\n",
    "plt.xlabel('Depth of Tree')\n",
    "\n",
    "plt.ylabel('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Stacking Ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Kaggle test set\n",
    "dropcols_train = ['YrMoSold', 'SalePrice', 'LogSalePrice']\n",
    "dropcols_dev = ['YrMoSold']\n",
    "\n",
    "def stack_ensemble(train_data, dev_data, cols_to_drop, model1, model2, model3):\n",
    "\n",
    "    # Get boosted predictions\n",
    "    best_boosted_model = model1\n",
    "    boosted_pred = best_boosted_model.predict(train_data.drop(columns=dropcols_train)) # how to include the cross validation here\n",
    "    boosted_test_pred = best_boosted_model.predict(dev_data.drop(columns=dropcols_dev))\n",
    "\n",
    "    # Get LR predictions\n",
    "    best_LR_model = model2\n",
    "    LR_pred = best_LR_model.predict(train_data.drop(columns=dropcols_train))\n",
    "    LR_test_pred = best_LR_model.predict(dev_data.drop(columns=dropcols_dev))\n",
    "\n",
    "    # Get bagged predictions\n",
    "    best_bagg_model = model3\n",
    "    bagg_pred = best_bagg_model.predict(train_data.drop(columns=dropcols_train))\n",
    "    bagg_test_pred = best_bagg_model.predict(dev_data.drop(columns=dropcols_dev))\n",
    "\n",
    "    # Create stacked model\n",
    "    trainpred = pd.DataFrame(np.column_stack((boosted_pred, LR_pred, bagg_pred)), columns = ['boosted', 'LR', 'bagg'])\n",
    "    testpred  = pd.DataFrame(np.column_stack((boosted_test_pred, LR_test_pred, bagg_test_pred)), columns = ['boosted', 'LR', 'bagg'])\n",
    "\n",
    "    # Fit the ensemble parameters\n",
    "    ensemble_LR = LinearRegression()\n",
    "    ensemble_LR.fit(trainpred, train_data['LogSalePrice'])\n",
    "    finaltrainpred = ensemble_LR.predict(trainpred)\n",
    "\n",
    "    # Fit the final predictions\n",
    "    finaltestpred = ensemble_LR.predict(testpred)\n",
    "\n",
    "    # Calculate the error (note this is the whole dataset so test/train are the same)\n",
    "    error = testing.calculate_error(train_true = np.array(train_data['LogSalePrice']), \n",
    "                            train_pred = np.array(finaltrainpred),\n",
    "                            test_true  = np.array(train_data['LogSalePrice']),\n",
    "                            test_pred  = np.array(finaltrainpred),\n",
    "                            outcome_var = 'LogSalePrice')\n",
    "\n",
    "    print(error)\n",
    "    print(finaltestpred.shape)\n",
    "    \n",
    "    return np.exp(finaltestpred)\n",
    "\n",
    "kagglepred = stack_ensemble(train_data = data,\n",
    "                              dev_data = kaggle_test_data,\n",
    "                              cols_to_drop = dropcols,\n",
    "                              model1 = df_boosting.sort_values('Root MSE')['Model'][4][0],\n",
    "                              model2 = lr_models.sort_values('Root MSE')['Model'][2][0],\n",
    "                              model3 = df.sort_values('Root MSE')['Model'][2][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testing.make_kaggle_submission(list(kagglepred),\n",
    "                               kaggle_test_data,\n",
    "                               'tempsubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Messing around w/ Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prep Data for Kaggle\n",
    "kaggle_lr = pd.DataFrame()\n",
    "kaggle_lr['Features'] = [feature_sets]\n",
    "kaggle_lr['Models'] = lr_models['Model']\n",
    "kaggle_lr['Num features in each'] = len(*feature_sets)\n",
    "kaggle_lr['Outcome_Vars'] = [['LogSalePrice', 'LogSalePrice', 'LogSalePrice']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 2 steps for submitting to kaggle:\n",
    "## retrain models on all data and get new preds\n",
    "## then make the final submission CSV\n",
    "final_preds = testing.retrain_on_all_data_and_get_final_preds(kaggle_lr, data, kaggle_test_data)\n",
    "\n",
    "testing.make_kaggle_submission(final_preds,\n",
    "                               kaggle_test_data,\n",
    "                               'tempsubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where this modeling section is going\n",
    "\n",
    "In our final report, we intend to include more sections of different typess of models and ensembles, and to build out the sections with some deeper explanations.\n",
    "\n",
    "Some things we intend to try:\n",
    "- Bayesian ridge models\n",
    "- K nearest neighbor\n",
    "- Boosting ensembles\n",
    "- Bagging ensembles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "In this section, we'll go into more detail about how we actually iterated on models and chose whichever ones we end up deciding are our best. Our primary tools will be this error correlation table, which we'll use to look at patterns of errors the model is making, and diagnostics to determine whether or not the model is overfitting. We'll compare different models to each other and explain the model or ensemble that gives us the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this still only works on individual models, it doens't average the correlations over a set of models\n",
    "# this tool is really more exploratory than anything - look at a couple models from the set you care about\n",
    "# and see what the trends are\n",
    "\n",
    "# use this variable to specify which model specification to use\n",
    "df_and_row_to_use = lrdf.iloc[0]\n",
    "# use this variable to specify which in the list of models trained with that specification to use\n",
    "model_to_use = df_and_row_to_use['Model'][1]\n",
    "# don't change this\n",
    "features_to_use = df_and_row_to_use['Features']\n",
    "\n",
    "def create_error_correlation_table(model,\n",
    "                                   outcome_var,\n",
    "                                   feature_set,\n",
    "                                   dev_df):\n",
    "    \n",
    "    '''\n",
    "    finds correlation between absolute value of error\n",
    "    and each feature\n",
    "    '''\n",
    "    \n",
    "    final_data = {'col': feature_set}\n",
    "    dev_df = dev_df.reset_index()\n",
    "    \n",
    "    dev_preds = model.predict(dev_df[feature_set])\n",
    "\n",
    "    rmsles = []\n",
    "    for i in range(len(dev_preds)):\n",
    "        rmsles.append(testing.rmsle([dev_df[outcome_var][i]], [dev_preds[i]]))\n",
    "\n",
    "    plt.clf()\n",
    "    plt.hist(rmsles, bins=20)\n",
    "    plt.xlabel(\"RMSLE\")\n",
    "    plt.ylabel(\"Number of Occurrences\")\n",
    "    plt.show()\n",
    "\n",
    "    dev_df['linear_reg_errors'] = rmsles\n",
    "\n",
    "    corrs = []\n",
    "    for col in feature_set:\n",
    "        try:\n",
    "            cor = np.corrcoef(abs(dev_df['linear_reg_errors']), dev_df[col])[0,1]\n",
    "            corrs.append(cor)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    final_data['correlation'] = corrs \n",
    "    \n",
    "    corrs_df = pd.DataFrame(data=final_data)\n",
    "    corrs_df = corrs_df.dropna()\n",
    "    return corrs_df\n",
    "  \n",
    "# table for our LR with all the features\n",
    "corrs_df = create_error_correlation_table(model_to_use, 'LogSalePrice', features_to_use, dev_df)\n",
    "corrs_df.reindex(corrs_df.correlation.abs().sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Errors by Key Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cycle through each model variation and plot errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest\n",
    "df.sort_values('Root MSE', ascending=True).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear Regression\n",
    "lrdf.sort_values('Root MSE', ascending=True).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### TODO YW: what do you want to use for feature importance here? save off the PCA feature importance\n",
    "### from the last notebook?\n",
    "\n",
    "### Random Forest Errors\n",
    "rf_error_spec = df.sort_values('Root MSE', ascending=True).iloc[0]\n",
    "model_to_use = rf_error_spec['Model'][0]\n",
    "features_to_use = rf_error_spec['Features']\n",
    "plot_features = list(feature_importances[:20].index)\n",
    "plot_error_against_var(model_to_use, 'LogSalePrice', features_to_use, plot_features, dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear Regression Errors\n",
    "# use this variable to specify which model specification to use\n",
    "df_and_row_to_use = lrdf.iloc[0]\n",
    "model_to_use = df_and_row_to_use['Model'][0]\n",
    "features_to_use = df_and_row_to_use['Features']\n",
    "plot_features = list(feature_importances[:20].index)\n",
    "plot_error_against_var(model_to_use, 'LogSalePrice', features_to_use, plot_features, dev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMILY - YOUR SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different models will perform better on different numbers of features. For that reason, it seems worthwhile to try all the individual models we're using on different numbers of features. It will likely be advantageous to ensemble models trained on different numbers of features. \n",
    "\n",
    "We'll use feature importances, determined via random forest, in order to decide which features to use. It may also be worthwhile to try some other feature selection methods, or to try random selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.read_csv('feature_importance.csv')\n",
    "feature_importances.columns = ['feature', 'importance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets_to_try = []\n",
    "\n",
    "for i in range(1,24):\n",
    "    set_to_try = list(feature_importances.feature.values)[:(i * 10)]\n",
    "    set_to_try = [item for item in set_to_try if item not in ['SalePrice', 'LogSalePrice']]\n",
    "    feature_sets_to_try.append(set_to_try)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_param_list = {LinearRegression: [{}]}\n",
    "outcome_vars = ['LogSalePrice', 'SalePrice']\n",
    "# for all models, we'll try with both the full feature set and the \"top 10\" feature set\n",
    "lrdf = model_train.try_different_models(cross_val_list, \n",
    "                                        models_to_param_list,\n",
    "                                        outcome_vars, \n",
    "                                        feature_sets_to_try)\n",
    "lrdf.sort_values('Root MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_param_list = {DecisionTreeRegressor: [{}], \n",
    "                        RandomForestRegressor: [{'n_estimators': 10},\n",
    "                                                {'min_samples_leaf': 3, 'n_estimators': 20}]}\n",
    "\n",
    "df = model_train.try_different_models(cross_val_list, \n",
    "                                     models_to_param_list,\n",
    "                                     outcome_vars, \n",
    "                                     feature_sets_to_try)\n",
    "df.sort_values('Root MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab bag of other models - BR and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_param_list = {BayesianRidge: [{}], \n",
    "                        KNeighborsRegressor: [{}]}\n",
    "\n",
    "gb_df = model_train.try_different_models(cross_val_list, \n",
    "                                         models_to_param_list,\n",
    "                                         outcome_vars, \n",
    "                                         feature_sets_to_try)\n",
    "gb_df.sort_values('Root MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_models = ensemble.get_model_dicts_from_data_frame(lrdf.sort_values('Root MSE')[:5])\n",
    "tree_models = ensemble.get_model_dicts_from_data_frame(df.sort_values('Root MSE')[:5])\n",
    "bayesian_models = ensemble.get_model_dicts_from_data_frame(gb_df.sort_values('Root MSE')[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_of_each = [models[0] for models in [lin_models, tree_models, bayesian_models]]\n",
    "lin_and_tree = [models[0] for models in [lin_models, tree_models]]\n",
    "lin_and_bayesian = [models[0] for models in [lin_models, bayesian_models]]\n",
    "bayesian_and_tree = [models[0] for models in [bayesian_models, tree_models]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_ensembles_df = ensemble.try_different_voting_ensembles(cross_val_list,\n",
    "                                                              [top_of_each,\n",
    "                                                               lin_and_tree,\n",
    "                                                               lin_and_bayesian,\n",
    "                                                               bayesian_and_tree, \n",
    "                                                               tree_models,\n",
    "                                                               lin_models,\n",
    "                                                               bayesian_models,\n",
    "                                                               tree_models[:2],\n",
    "                                                               tree_models[:3],\n",
    "                                                               tree_models[:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_ensembles_df.sort_values('RMSE for ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose model - for now, just choosing the best one\n",
    "ensemble_to_use = voting_ensembles_df.sort_values('RMSE for ensemble')[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 steps for submitting to kaggle:\n",
    "## retrain models on all data and get new preds\n",
    "## then make the final submission CSV\n",
    "final_preds = testing.retrain_on_all_data_and_get_final_preds(ensemble_to_use,\n",
    "                                                              data,\n",
    "                                                              kaggle_test_data)\n",
    "testing.make_kaggle_submission(final_preds,\n",
    "                               kaggle_test_data,\n",
    "                               'tempsubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ensemble_to_use['Outcome_Vars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
