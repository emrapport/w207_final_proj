{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Baseline\n",
    "Yang Wei Neo, Emily Rapport, Hilary Yamtich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "from rfpimp import *\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "# note: this notebook requires pandas 0.21.0 or newer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import testing_utils as testing\n",
    "import model_training_utils as model_train\n",
    "import ensemble_model_utils as ensemble\n",
    "import math\n",
    "from datetime import datetime as dt\n",
    "import re as re\n",
    "import pickle as pk\n",
    "import logging\n",
    "\n",
    "# For producing decision tree diagrams.\n",
    "from IPython.core.display import Image, display\n",
    "from sklearn.externals.six import StringIO\n",
    "\n",
    "from dateutil import parser\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the pickle file that contains the clean data and other useful stuff?\n",
    "infile = open('./clean_data_pickle','rb')\n",
    "data = pk.load(infile)\n",
    "infile.close()\n",
    "\n",
    "with open('clean_test_data.pkl', 'rb') as infile:\n",
    "    kaggle_test_data = pk.load(infile)\n",
    "    \n",
    "feature_importances = pd.read_csv('feature_importance.csv')\n",
    "feature_importances.columns = ['feature', 'importance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with a simple 15% dev set, but we have found that for this amount of data, the differences in the models and their scores on the dev sets can vary significantly based on which rows end up in the train and dev sets. Repeated random sub-sampling cross validation helps us get more consistent results.\n",
    "\n",
    "Note that we do not split out the dev data using the most recent years, which would be the proper way to create a dev set if our task were explicitly to predict future home prices. The test data appears to have rows from all the years represented in the train set, so we built dev sets that sample from across the train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# still to do : choose one version of pandas to use so that our code all agrees\n",
    "# and I don't have to read in a new dataset here \n",
    "NUM_CROSS_VALS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the list of different cross val splits\n",
    "cross_val_list = []\n",
    "for i in range(NUM_CROSS_VALS):\n",
    "    split_idx = int(data.shape[0] * .85)\n",
    "    # line below is what shuffles\n",
    "    data = data.sample(frac=1)\n",
    "    train_df = data[:split_idx]\n",
    "    dev_df = data[split_idx:]\n",
    "    split_dict = {'train_df': train_df,\n",
    "                  'dev_df': dev_df}\n",
    "    cross_val_list.append(split_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO YW - TALK ABOUT ERROR METRIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Baseline\n",
    "\n",
    "\n",
    "As our primary error metric, we focus on the root mean squared error of the logarithm of the prices, which is the error metric being used to create the leaderboard for this kaggle competition. See rmsle() in shared_functions.py for our implementation of the root mean squared error, an implementation we found from Mark Nagelberg on Kaggle: https://www.kaggle.com/marknagelberg/rmsle-function.\n",
    "\n",
    "When we consulted our resident real estate expert, Hilary's dad, about this problem, he told us that only one of these factors matters - \"location, location, location.\" In the spirit of that insight, we created a baseline \"model\" which looks at what neighborhood the house is in and takes the mean price of houses from that neighborhood in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# todo: figure out why i'm getting nans now\n",
    "# when i wasn't in original notebook\n",
    "def baseline_pred(row,\n",
    "                  train_df):\n",
    "    for col in train_df:\n",
    "        if 'Neighborhood' in col:\n",
    "            if row[col] == 1:\n",
    "                neighborhood_var = col\n",
    "                break\n",
    "    return np.nanmean(train_df[train_df[neighborhood_var]==1]['LogSalePrice'])\n",
    "\n",
    "def get_baseline_cross_val(cross_val_list):\n",
    "    all_rmses = []\n",
    "    for di in cross_val_list:\n",
    "        dev_df = di['dev_df']\n",
    "        dev_df['baseline_pred'] = dev_df.apply(lambda row: baseline_pred(row,\n",
    "                                                                         di['train_df']), axis=1)\n",
    "        rmse = testing.rmsle(list(np.exp(dev_df['LogSalePrice'])), list(np.exp(dev_df['baseline_pred'])))\n",
    "        all_rmses.append(rmse)\n",
    "    return np.nanmean(all_rmses) \n",
    "\n",
    "# baseline RMSLE\n",
    "print(\"Baseline RMSLE: {:.3f}\".format(get_baseline_cross_val(cross_val_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this as a baseline, we began exploring how different types of models perform on the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Exploration \n",
    "#### Linear Regression\n",
    "\n",
    "We begin with linear regression as the standard choice for a regression problem. In ordinary least squares regression, the regression line is fit by minimizing the sum of squared residuals between the predicted line and the true data points. We can interpret the resulting coefficients on each feature as representing the additional impact of a one-unit change in that feature on the final price.\n",
    "\n",
    "- Will not work well with variables that are highly correlated with each other\n",
    "- Indeed this is why linear regression with PCA seems to do just as well as the original variables; the  loss is not as great for linear regression (only 0.04 loss in RMSLE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up different LR models\n",
    "models_to_param_list = {LinearRegression: [{}], \n",
    "                        Lasso: [{'alpha': 0.005}], # lower value is less regularization\n",
    "                        Ridge: [{'alpha': 2}]} # more effective with more regularization\n",
    "                        #ElasticNet: [{'alpha': 0.1}]} # lower value is less regularization\n",
    "\n",
    "# Outcome \n",
    "outcome_vars = ['LogSalePrice']\n",
    "\n",
    "# Create feature sets\n",
    "feature_sets = [[col for col in data.columns if col not in ['YrMoSold', 'LogSalePrice', 'SalePrice']]]\n",
    "\n",
    "# Output model\n",
    "lr_models = model_train.try_different_models(cross_val_list, \n",
    "                                        models_to_param_list,\n",
    "                                        outcome_vars, \n",
    "                                        feature_sets)\n",
    "\n",
    "lr_models.sort_values('Root MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Illustration\n",
    "\n",
    "Bagging, or bootstrap aggregation, is intended to reduce variance in the test error by averaging predictions over very specialized models. While each of these models in isolation is likely to overfit, the ensemble of specialized models ends up being very effective at reducing overall test error. To stress test this assumption, we run several random forest ensembles on models that are increasingly less likely to *individually* overfit. We find that the more likely each individual model is to overfit (either by enforcing a smaller minimum leaf, or by enforcing a higher split size), the lower the error of the ensemble as a whole. We suspect that this is because a large ensemble paired with high variance/low bias individual models gives the best of both worlds.\n",
    "\n",
    "Note however that this phenomenon is not true when we increase the proportion of features used to split at each node (causing any given individual tree to be more likely to overfit). We aren't sure why... !!!\n",
    "\n",
    "Another interesting phenomenon is that the difference between the training and test error decreases as each individual model within the ensemble gets less complex. This is a sign of increasing bias in the underlying model, which makes sense, since each model is more likely to underfit since it has less underlying complexity. \n",
    "\n",
    "This analysis suggests that we ought to lean towards creating more complex individual trees (low bias); the higher variance that results from this ought be offset by the bootstrap aggregation procedure. We will use a grid search to find the optimal combination of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize list of tests:\n",
    "param_list = []\n",
    "\n",
    "# Create list of parameter types\n",
    "for min_leaf_size in range(10):\n",
    "    param_list.append({'min_samples_leaf': min_leaf_size, 'n_estimators': 50})\n",
    "    \n",
    "for feature_prop in range(10):\n",
    "    param_list.append({'max_features': feature_prop/10, 'n_estimators': 50})\n",
    "                       \n",
    "for split_size in range(11):\n",
    "    param_list.append({'min_samples_split': split_size, 'n_estimators': 50})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models to show the impact of bagging\n",
    "### THIS TAKES A LONG TIME TO RUN\n",
    "models_to_param_list = {RandomForestRegressor: param_list}\n",
    "feature_sets = [[col for col in data.columns if col not in ['YrMoSold', 'LogSalePrice', 'SalePrice']]]\n",
    "\n",
    "# Run different random forests\n",
    "df = model_train.try_different_models(cross_val_list, \n",
    "                             models_to_param_list,\n",
    "                             outcome_vars, \n",
    "                             feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "fig, ax = plt.subplots(1,3, sharey='row')\n",
    "fig.tight_layout(pad = 1.5)\n",
    "\n",
    "ax[0].plot(df.iloc[0:9]['Root MSE'])\n",
    "ax[0].plot(df.iloc[0:9]['Train MSE'])\n",
    "ax[0].set_xlabel('Min Leaf Size')\n",
    "\n",
    "ax[1].plot(df.iloc[10:20]['Root MSE'])\n",
    "ax[1].plot(df.iloc[10:20]['Train MSE'])\n",
    "ax[1].set_xlabel('% of Features used to split')\n",
    "\n",
    "ax[2].plot(df.iloc[20:30]['Root MSE'])\n",
    "ax[2].plot(df.iloc[20:30]['Train MSE'])\n",
    "ax[2].set_xlabel('Minimum Split Size')\n",
    "\n",
    "fig.suptitle('Bagging with High Variance Models')\n",
    "# !!! remove the axis tick marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Boosting Illustration\n",
    "\n",
    "In contrast, boosting is a process that reduces bias by refitting the model iteratively on the errors from the previous model. Boosting can turn weak learners into a accurate ensemble - we can see this below by showing how a relatively weak learner (tree of depth 3) actually appears to have the best accuracy in a GBM as compared to a GBM with greater depth. However, GBMs do have a tendency towards high variance (as seen by how quickly the model overfits the training data relative to the test data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize list of tests:\n",
    "param_list = []\n",
    "# Create list of parameter types\n",
    "for depth in range(20):\n",
    "    param_list.append({'max_depth': depth, 'n_estimators': 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run Gradient Boosting Results\n",
    "models_to_param_list = {GradientBoostingRegressor: param_list}\n",
    "df_boosting = model_train.try_different_models(cross_val_list, \n",
    "                             models_to_param_list,\n",
    "                             outcome_vars, \n",
    "                             feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the DF Boosting Results.\n",
    "plt.plot(df_boosting.index, df_boosting['Root MSE'])\n",
    "plt.plot(df_boosting.index, df_boosting['Train MSE'])\n",
    "plt.xlabel('Depth of Tree')\n",
    "\n",
    "plt.ylabel('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Ridge \n",
    "TODO EMILY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian ridge is a form of ridge regression, which imposes a penalty on the size of the coefficients. In the Bayesian form of ridge regression, the parameters are estimated using a Gaussian prior. The coefficients, as well as the parameters of the Gaussian distribution (mean and variance) are estimated from the data using maximum likelihood estimation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## see how shape of coefficients changes as number of iterations goes up \n",
    "=param_list = []\n",
    "for num_iter in range(1, 40):\n",
    "    param_list.append({'n_iter': num_iter})\n",
    "    \n",
    "models_to_param_dict = {BayesianRidge: param_list}\n",
    "\n",
    "df = model_train.try_different_models(cross_val_list, \n",
    "                                      models_to_param_dict,\n",
    "                                      ['SalePrice'], \n",
    "                                      feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [models[0] for models in df['Model'].values]\n",
    "coefficients = [model.coef_ for model in models]\n",
    "\n",
    "# todo: make this nice with subplots \n",
    "for i in range(0, len(models), 2):\n",
    "    plt.hist(coefficients[i])\n",
    "    plt.title(\"Iteration {}\".format(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We experimented with a K Neighbors Regressor, which identifies the k nearest neighbors of the given example and averages their target variables in order to obtain a prediction. A challenge of the K Neighbors algorithm is that it is not able to learn relative importance of different features; as a result, it struggles as the number of features increases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_feature_sets_to_try = []\n",
    "\n",
    "for i in range(1,75):\n",
    "    set_to_try = list(feature_importances.feature.values)[:(i)]\n",
    "    set_to_try = [item for item in set_to_try if item not in ['SalePrice', 'LogSalePrice']]\n",
    "    k_feature_sets_to_try.append(set_to_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to do: would also be fun to see how this changes as k changes, but maybe not that important  \n",
    "models_to_param_dict = {KNeighborsRegressor: [{}]}\n",
    "\n",
    "k_df = model_train.try_different_models(cross_val_list, \n",
    "                                        models_to_param_dict,\n",
    "                                        ['SalePrice'], \n",
    "                                         k_feature_sets_to_try)\n",
    "k_df.plot.scatter(x='Num Features', y='Root MSE', title=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows how the root MSE changes as the number of features changes. As the first few features get added in, there's significant instability in the error, as each new feature drastically changes the estimation of which data point is closest. We see that after the error stabilizes, the general trend is an increase in error as more features are added, which makes sense, since the features that get added later on are less important, and this model has no way of handling less inportant features. Additionally, we see significant periods of plateau, where new features do not cause any change in error. This is likely due to the sparsity of our data set, since many of our features are dummy variables for different values of categories, and those variables are often very sparse (many 0 values across the data set). The sheer number of features, combined with the relative sparsity of our set, make K Neighbors a poor choice for this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Ensembling Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting & Avergaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMILY TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first, simple pass at ensembling, we tried different variations of voting ensembles where each preciction is an average of the predictions of individual models in the ensemble. The intuition here is that different types of models will error in different types of ways, so if one type of model - say, a linear model - has a bias that disproportionately causes errors on certain types of rows, then averaging those predictions with that of a model - say, tree-based - that better handles those rows would pull bad predictions back in the right direction. \n",
    "\n",
    "(here, it would be good to include a little play-by-play of ensembles we tried and how our kaggle score rose with each one. maybe also compare them on our own dev sets to see if they match...? I don't think they do...). \n",
    "\n",
    "We used a combination of experimentation and theory to guide our selection of different voting ensembles. We watched our Kaggle leaderboard position go up as we made changes to these ensembles, and learned the following:\n",
    "- **Linear + tree-based ensembles work well overall**: We know that linear models tend to have errors born out of bias in the specification, whereas tree-based models tend to have errors born out of variance. It makes sense that averaging predictions from these types of models would help counteract the errors specific to each type. \n",
    "- **Weighting the averages towards the linear models (either with a weighting average, or by including more linear models) tends to work well**: Our linear models tend to predict better than our tree-based ensembles overall, so weighting our ensembles towards linear models helped us capture the best predictions. In general, including multiple linear models works better than overly weighting the predictions of one linear model; this makes sense, as when we included different linear models, we tried to choose models whose errors were not correlated with the same variables in the hopes that bad predictions from one model would be balanced out by good predictions from the other model on the same example, and vice versa. (would be good to show error correlation of a bayesian vs a linear from the same ensemble)\n",
    "- **Linear models best predict the logarithmic outcome variable, whereas tree-based models best predict the \"true\" outcome variable:** We looked at all types of models trained on both types of outcome variables, and in general, we found that each model was best suited to a particular outcome variable in a way that made intuitive sense to us. Since the linear models are confined to linear decision boundaries in making their predictions, they do best predicting on a normally-distributed variable, since outliers are not well-handled by linear boundaries. The logaritmic outcome variable was more normally distributed than the normal outcome variable, so the linear models handled this well. On the other hand, the tree-based models can handle non-linear decision boundaries, as \"outlier\" type predictions can simply be handled by particular branches in the trees. The non-logarithmic outcome variable preserves more of the variation in the outcome variable (i.e. it makes examples with similar SalePrices look more different than they would look if you took their logarithm), so using the non-logarithmic variable allows the tree-based model to capture more nuanced decision rules that capture the variability in the underlying data.  (note - I'm not actually sure if this point belongs in the ensembling section or somewhere else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking\n",
    "YW TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Kaggle test set\n",
    "dropcols_train = ['YrMoSold', 'SalePrice', 'LogSalePrice']\n",
    "dropcols_dev = ['YrMoSold']\n",
    "\n",
    "def stack_ensemble(train_data, dev_data, cols_to_drop, model1, model2, model3):\n",
    "\n",
    "    # Get boosted predictions\n",
    "    best_boosted_model = model1\n",
    "    boosted_pred = best_boosted_model.predict(train_data.drop(columns=dropcols_train)) # how to include the cross validation here\n",
    "    boosted_test_pred = best_boosted_model.predict(dev_data.drop(columns=dropcols_dev))\n",
    "\n",
    "    # Get LR predictions\n",
    "    best_LR_model = model2\n",
    "    LR_pred = best_LR_model.predict(train_data.drop(columns=dropcols_train))\n",
    "    LR_test_pred = best_LR_model.predict(dev_data.drop(columns=dropcols_dev))\n",
    "\n",
    "    # Get bagged predictions\n",
    "    best_bagg_model = model3\n",
    "    bagg_pred = best_bagg_model.predict(train_data.drop(columns=dropcols_train))\n",
    "    bagg_test_pred = best_bagg_model.predict(dev_data.drop(columns=dropcols_dev))\n",
    "\n",
    "    # Create stacked model\n",
    "    trainpred = pd.DataFrame(np.column_stack((boosted_pred, LR_pred, bagg_pred)), columns = ['boosted', 'LR', 'bagg'])\n",
    "    testpred  = pd.DataFrame(np.column_stack((boosted_test_pred, LR_test_pred, bagg_test_pred)), columns = ['boosted', 'LR', 'bagg'])\n",
    "\n",
    "    # Fit the ensemble parameters\n",
    "    ensemble_LR = LinearRegression()\n",
    "    ensemble_LR.fit(trainpred, train_data['LogSalePrice'])\n",
    "    finaltrainpred = ensemble_LR.predict(trainpred)\n",
    "\n",
    "    # Fit the final predictions\n",
    "    finaltestpred = ensemble_LR.predict(testpred)\n",
    "\n",
    "    # Calculate the error (note this is the whole dataset so test/train are the same)\n",
    "    error = testing.calculate_error(train_true = np.array(train_data['LogSalePrice']), \n",
    "                            train_pred = np.array(finaltrainpred),\n",
    "                            test_true  = np.array(train_data['LogSalePrice']),\n",
    "                            test_pred  = np.array(finaltrainpred),\n",
    "                            outcome_var = 'LogSalePrice')\n",
    "\n",
    "    print(error)\n",
    "    print(finaltestpred.shape)\n",
    "    \n",
    "    return np.exp(finaltestpred)\n",
    "\n",
    "kagglepred = stack_ensemble(train_data = data,\n",
    "                              dev_data = kaggle_test_data,\n",
    "                              cols_to_drop = dropcols,\n",
    "                              model1 = df_boosting.sort_values('Root MSE')['Model'][4][0],\n",
    "                              model2 = lr_models.sort_values('Root MSE')['Model'][2][0],\n",
    "                              model3 = df.sort_values('Root MSE')['Model'][2][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "testing.make_kaggle_submission(list(kagglepred),\n",
    "                               kaggle_test_data,\n",
    "                               'tempsubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "In this section, we'll go into more detail about how we actually iterated on models and chose whichever ones we end up deciding are our best. Our primary tools will be this error correlation table, which we'll use to look at patterns of errors the model is making, and diagnostics to determine whether or not the model is overfitting. We'll compare different models to each other and explain the model or ensemble that gives us the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this still only works on individual models, it doens't average the correlations over a set of models\n",
    "# this tool is really more exploratory than anything - look at a couple models from the set you care about\n",
    "# and see what the trends are\n",
    "\n",
    "# use this variable to specify which model specification to use\n",
    "#df_and_row_to_use = lrdf[:1]\n",
    "# use this variable to specify which in the list of models trained with that specification to use\n",
    "#model_to_use = df_and_row_to_use['Model'][0]\n",
    "# don't change this\n",
    "#features_to_use = df_and_row_to_use['Features']\n",
    "\n",
    "lr_to_eval = ensemble.get_model_dicts_from_data_frame(lrdf[:1])\n",
    "print(lr_to_eval)\n",
    "\n",
    "# table for our LR with all the features\n",
    "corrs_df = testing.create_error_correlation_table(lr_to_eval, dev_df)\n",
    "corrs_df.reindex(corrs_df.avg_correlation.abs().sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cycle through each model variation and plot errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Random Forest\n",
    "df.sort_values('Root MSE', ascending=True).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Linear Regression\n",
    "lrdf.sort_values('Root MSE', ascending=True).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### TODO YW: what do you want to use for feature importance here? save off the PCA feature importance\n",
    "### from the last notebook?\n",
    "\n",
    "### Random Forest Errors\n",
    "rf_error_spec = df.sort_values('Root MSE', ascending=True).iloc[0]\n",
    "model_to_use = rf_error_spec['Model'][0]\n",
    "features_to_use = rf_error_spec['Features']\n",
    "plot_features = list(feature_importances[:20].index)\n",
    "plot_error_against_var(model_to_use, 'LogSalePrice', features_to_use, plot_features, dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Linear Regression Errors\n",
    "# use this variable to specify which model specification to use\n",
    "df_and_row_to_use = lrdf.iloc[0]\n",
    "features_to_use = df_and_row_to_use['Features']\n",
    "plot_features = list(feature_importances[:20].index)\n",
    "plot_error_against_var(model_to_use, 'LogSalePrice', features_to_use, plot_features, dev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model & Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different models will perform better on different numbers of features. For that reason, it seems worthwhile to try all the individual models we're using on different numbers of features. It will likely be advantageous to ensemble models trained on different numbers of features. \n",
    "\n",
    "We'll use feature importances, determined via random forest, in order to decide which features to use. It may also be worthwhile to try some other feature selection methods, or to try random selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_sets_to_try = []\n",
    "\n",
    "for i in range(5,24):\n",
    "    set_to_try = list(feature_importances.feature.values)[:(i * 10)]\n",
    "    set_to_try = [item for item in set_to_try if item not in ['SalePrice', 'LogSalePrice']]\n",
    "    feature_sets_to_try.append(set_to_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Narrative of our process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### discussion of the scikitlearn column - real working problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### first we worked just in this notebook for a long time, trying different models only on cross-validated dev sets\n",
    "### then, we started "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1 Big Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Comparing model performance across different number of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Messing Around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prep Data for Kaggle\n",
    "kaggle_lr = pd.DataFrame()\n",
    "kaggle_lr['Features'] = [feature_sets]\n",
    "kaggle_lr['Models'] = lr_models['Model']\n",
    "kaggle_lr['Num features in each'] = len(*feature_sets)\n",
    "kaggle_lr['Outcome_Vars'] = [['LogSalePrice', 'LogSalePrice', 'LogSalePrice']]\n",
    "\n",
    "## 2 steps for submitting to kaggle:\n",
    "## retrain models on all data and get new preds\n",
    "## then make the final submission CSV\n",
    "final_preds = testing.retrain_on_all_data_and_get_final_preds(kaggle_lr, data, kaggle_test_data)\n",
    "\n",
    "testing.make_kaggle_submission(final_preds,\n",
    "                               kaggle_test_data,\n",
    "                               'tempsubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMILY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_to_eval = ensemble.get_model_dicts_from_data_frame(lrdf[:1])\n",
    "print(lr_to_eval)\n",
    "\n",
    "# table for our LR with all the features\n",
    "corrs_df = testing.create_error_correlation_table(lr_to_eval, dev_df)\n",
    "corrs_df.reindex(corrs_df.avg_correlation.abs().sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_to_eval = ensemble.get_model_dicts_from_data_frame(boost_df[11:12])\n",
    "print(lr_to_eval)\n",
    "\n",
    "# table for our LR with all the features\n",
    "corrs_df = testing.create_error_correlation_table(lr_to_eval, dev_df)\n",
    "corrs_df.reindex(corrs_df.avg_correlation.abs().sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_param_list = {GradientBoostingRegressor: [{}]}\n",
    "outcome_vars = ['LogSalePrice', 'SalePrice']\n",
    "# for all models, we'll try with both the full feature set and the \"top 10\" feature set\n",
    "boost_df = model_train.try_different_models(cross_val_list, \n",
    "                                        models_to_param_list,\n",
    "                                        outcome_vars, \n",
    "                                        feature_sets_to_try)\n",
    "boost_df = boost_df.sort_values('Root MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_df_no_log = boost_df[boost_df['Outcome Var'] == 'SalePrice']\n",
    "boost_df_no_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_param_list = {LinearRegression: [{}]}\n",
    "outcome_vars = ['LogSalePrice', 'SalePrice']\n",
    "# for all models, we'll try with both the full feature set and the \"top 10\" feature set\n",
    "lrdf = model_train.try_different_models(cross_val_list, \n",
    "                                        models_to_param_list,\n",
    "                                        outcome_vars, \n",
    "                                        feature_sets_to_try)\n",
    "lrdf = lrdf.sort_values('Root MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_param_list = {DecisionTreeRegressor: [{}], \n",
    "                        RandomForestRegressor: [{'n_estimators': 20},\n",
    "                                                {'min_samples_leaf': 3, 'n_estimators': 20}]}\n",
    "\n",
    "df = model_train.try_different_models(cross_val_list, \n",
    "                                     models_to_param_list,\n",
    "                                     outcome_vars, \n",
    "                                     feature_sets_to_try)\n",
    "df.sort_values('Root MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab bag of other models - BR and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_param_list = {BayesianRidge: [{}]}\n",
    "\n",
    "gb_df = model_train.try_different_models(cross_val_list, \n",
    "                                         models_to_param_list,\n",
    "                                         outcome_vars, \n",
    "                                         feature_sets_to_try)\n",
    "gb_df.sort_values('Root MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_models = ensemble.get_model_dicts_from_data_frame(lrdf.sort_values('Root MSE')[:10])\n",
    "#tree_models = ensemble.get_model_dicts_from_data_frame(df.sort_values('Root MSE')[:10])\n",
    "bayesian_models = ensemble.get_model_dicts_from_data_frame(gb_df.sort_values('Root MSE')[:20])\n",
    "boost_models = ensemble.get_model_dicts_from_data_frame(boost_df_no_log.sort_values('Root MSE')[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(boost_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_of_each = [models[0] for models in [lin_models, tree_models, bayesian_models]]\n",
    "lin_and_tree = [models[0] for models in [lin_models, tree_models]]\n",
    "lin_and_bayesian = [models[0] for models in [lin_models, bayesian_models]]\n",
    "bayesian_and_tree = [models[0] for models in [bayesian_models, tree_models]]\n",
    "lin_0_tree_6 = [lin_models[0], tree_models[6]]\n",
    "bayesian_0_tree_6 = [bayesian_models[0], tree_models[6]]\n",
    "top_of_b_and_l_plus_tree_8 = [bayesian_models[0], lin_models[0], tree_models[6]]\n",
    "same_but_boost_instead = [bayesian_models[0], lin_models[0], boost_models[28]]\n",
    "same_but_top_boost = [bayesian_models[0], lin_models[0], boost_models[0]]\n",
    "same_but_top_non_log_boost = [bayesian_models[0], lin_models[0], boost_models[11]]\n",
    "\n",
    "'''\n",
    "[top_of_each,\n",
    "                                                               lin_and_tree,\n",
    "                                                               lin_and_bayesian,\n",
    "                                                               bayesian_and_tree, \n",
    "                                                               lin_models,\n",
    "                                                               bayesian_models,\n",
    "                                                               lin_0_tree_6,\n",
    "                                                               bayesian_0_tree_6, \n",
    "                                                               top_of_b_and_l_plus_tree_8,\n",
    "                                                               same_but_boost_instead,\n",
    "                                                               same_but_top_boost,\n",
    "                                                               same_but_top_non_log_boost]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_ensembles_df = ensemble.try_different_voting_ensembles(cross_val_list,\n",
    "                                                              [[bayesian_models[0], bayesian_models[17],\n",
    "                                                                lin_models[0], boost_models[2]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_ensembles_df.sort_values('RMSE for ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose model - for now, just choosing the best one\n",
    "ensemble_to_use = voting_ensembles_df.sort_values('RMSE for ensemble')[:1]\n",
    "ensemble_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 2 steps for submitting to kaggle:\n",
    "## retrain models on all data and get new preds\n",
    "## then make the final submission CSV\n",
    "final_preds = testing.retrain_on_all_data_and_get_final_preds(ensemble_to_use,\n",
    "                                                              data,\n",
    "                                                              kaggle_test_data)\n",
    "testing.make_kaggle_submission(final_preds,\n",
    "                               kaggle_test_data,\n",
    "                               'em_sunday_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
