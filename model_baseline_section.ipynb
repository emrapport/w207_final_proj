{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import Binarizer, OneHotEncoder, OrdinalEncoder, add_dummy_feature\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from shared_functions import rmsle, train_and_test, try_different_models\n",
    "import math\n",
    "import graphviz\n",
    "\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "pd.set_option('precision', 4)\n",
    "pd.options.display.float_format = '{:.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(r\"clean_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df['LogSalePrice'] = np.log(data_df['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CROSS_VALS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we do a simple split of 10% of our data into a dev set. We also incorporate cross-validation, as we have found that for this amount of data, the differences in the models and their scores on the dev sets can vary significantly based on which rows end up in the train and dev sets. Repeated random sub-sampling cross validation helps us get more consistent r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_list = []\n",
    "for i in range(NUM_CROSS_VALS):\n",
    "    split_idx = int(data_df.shape[0] * .85)\n",
    "    # line below is what shuffles\n",
    "    data_df = data_df.sample(frac=1)\n",
    "    train_df = data_df[:split_idx]\n",
    "    dev_df = data_df[split_idx:]\n",
    "    split_dict = {'train_df': train_df,\n",
    "                  'dev_df': dev_df}\n",
    "    cross_val_list.append(split_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our primary error metric, we focus on the root mean squared error of the logarithm of the prices, which is the error metric being used to create the leaderboard for this kaggle competition. See rmsle() in shared_functions.py for our implementation of the root mean squared error, an implementation we found (where)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we consulted our resident real estate expert, Hilary's dad, about this problem, he told us that only one of these factors matters - \"location, location, location.\" In the spirit of that insight, we created a baseline \"model\" which looks at what neighborhood the house is in and takes the mean price of houses from that neighborhood in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_pred(row,\n",
    "                  train_df):\n",
    "    for col in train_df:\n",
    "        if 'Neighborhood' in col:\n",
    "            if row[col] == 1:\n",
    "                neighborhood_var = col\n",
    "                break\n",
    "    return np.mean(train_df[train_df[neighborhood_var]==1]['LogSalePrice'])\n",
    "\n",
    "def get_baseline_cross_val(cross_val_list):\n",
    "    all_rmses = []\n",
    "    for di in cross_val_list:\n",
    "        dev_df = di['dev_df']\n",
    "        dev_df['baseline_pred'] = dev_df.apply(lambda row: baseline_pred(row,\n",
    "                                                                         di['train_df']), axis=1)\n",
    "        rmse = rmsle(list(np.exp(dev_df['LogSalePrice'])), list(np.exp(dev_df['baseline_pred'])))\n",
    "        all_rmses.append(rmse)\n",
    "    return np.mean(all_rmses) \n",
    "\n",
    "# baseline RMSLE\n",
    "print(\"Baseline RMSLE: {:.3f}\".format(get_baseline_cross_val(cross_val_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this as a baseline, we began exploring how different types of models perform on the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: should we add a parsimony metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with linear regression as the standard choice for a regression problem. In ordinary least squares regression, the regression line is fit by minimizing the sum of squared residuals between the predicted line and the true data points. We can interpret the resulting coefficients on each feature as representing the additional impact of a one-unit change in that feature on the final price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = [LinearRegression()]\n",
    "outcome_vars = ['LogSalePrice']\n",
    "feature_sets = [[col for col in data_df.columns if col not in ['YrMoSold', 'LogSalePrice', 'SalePrice'] and \n",
    "                 'Condition2' not in col],\n",
    "                [col for col in data_df.columns if col not in ['YrMoSold', 'LogSalePrice', 'SalePrice', 'OverallQual_10'] and \n",
    "                 'Condition2' not in col],\n",
    "               [col for col in data_df.columns if col not in ['YrMoSold', 'LogSalePrice', 'SalePrice']]]\n",
    "lrdf = try_different_models(cross_val_list, models, outcome_vars, feature_sets)\n",
    "lrdf.sort_values('Root MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ to explain: normalize makes this way worse - I'm guessing that's because of how many dummy cols we have where the mean overall is going to skew to 0? but then the next time, normalize stayed the same....ugh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree-Based Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The family of tree-based regressors learns a series of simple decision rules to predict the final sale price. The decision tree regressor makes one single Decision Tree, whereas the Random Forest regressor trains an ensemble of decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [DecisionTreeRegressor(), RandomForestRegressor()]\n",
    "df = try_different_models(cross_val_list, models, outcome_vars, feature_sets)\n",
    "df[['Model', 'Num Features', 'Outcome Var', 'Root MSE']].sort_values('Root MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest regressor shows up in the table as the DecisionTreeRegressor with the parentheses around the whole function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's saying the model in the table isn't fit yet, which doesn't really make sense to me\n",
    "# need to figure that out\n",
    "'''dot_data = export_graphviz(df.iloc[1]['Model'], out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"housing\") '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [BayesianRidge()]\n",
    "df = try_different_models(cross_val_list, models, outcome_vars, feature_sets)\n",
    "df[['Model', 'Num Features', 'Outcome Var', 'Root MSE']].sort_values('Root MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More sections with particular model types, explanations, visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "In this section, we'll go into more detail about how we actually iterated on models and chose whichever ones we end up deciding are our best. Our primary tools will be this error correlation table, which we'll use to look at patterns of errors the model is making, and diagnostics to determine whether or not the model is overfitting. We'll compare different models to each other and explain the model or ensemble that we chose as our \"best.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = lrdf.iloc[0]['Model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_error_correlation_table(model,\n",
    "                                   outcome_var,\n",
    "                                   feature_set,\n",
    "                                   dev_df):\n",
    "    \n",
    "    '''\n",
    "    finds correlation between absolute value of error\n",
    "    and each feature\n",
    "    '''\n",
    "    \n",
    "    dev_preds = model.predict(dev_df[feature_set])\n",
    "    dev_df = dev_df.reset_index()\n",
    "    \n",
    "    rmsles = []\n",
    "    for i in range(len(dev_preds)):\n",
    "        rmsles.append(rmsle([dev_df[outcome_var][i]], [dev_preds[i]]))\n",
    "        \n",
    "    plt.hist(rmsles, bins=20)\n",
    "    plt.xlabel(\"RMSLE\")\n",
    "    plt.ylabel(\"Number of Occurrences\")\n",
    "    plt.show()\n",
    "    \n",
    "    dev_df['linear_reg_errors'] = rmsles\n",
    "    \n",
    "    cols = []\n",
    "    corrs = []\n",
    "    for col in dev_df.columns:\n",
    "        try:\n",
    "            cor = np.corrcoef(abs(dev_df['linear_reg_errors']), dev_df[col])[0,1]\n",
    "            cols.append(col)\n",
    "            corrs.append(cor)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    corrs_df = pd.DataFrame(data={'col': cols, 'correlation': corrs})\n",
    "    corrs_df = corrs_df.dropna(subset=['correlation'])\n",
    "    return corrs_df\n",
    "    \n",
    "corrs_df = create_error_correlation_table(lr, 'LogSalePrice', feature_sets[-1], dev_df)\n",
    "corrs_df.reindex(corrs_df.correlation.abs().sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Third section: interpretability, deeper dive into certain use cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
